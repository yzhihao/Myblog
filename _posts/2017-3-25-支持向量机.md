---
layout: post
title: 支持向量机（SVM）
desc: 我的博客
keywords: 'blog,Machine Learning,AI'
date: 2017-3-25T00:00:00.000Z
categories:
  - Machine Learning
tags:
  - Machine Learning
  - AI
icon: fa-book
---


## 目录
**欢迎在文章下方评论，建议用电脑看**

* 目录
{:toc}

# 支持向量机（SVM）

今天讲下支持向量机！这个模型很强大，但理解起来可能有点麻烦，它的理论支持已经比较完善了（相对于深度神经网络而言），涉及的数学比较多，我理解的也相当的有限，希望能够讲好点，下面开始吧！


## 基本概念

* 支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。
* 按照惯例，先上一个最通俗的例子来直观的理解SVM，我从知乎上找到了一篇回答，他很形象的描述了SVM，传送门：[通俗理解svm](https://www.zhihu.com/question/21094489)
* 最大间隔分类器（maximum margin classifier，可以被看做是支持向量机的前身）实际上就选择特定的w,b使**几何间隔**最大化。
* 它通常被认为是最好的现成监督学习算法之一（很多人认为它是最好的）

## 和Logistic回归的区别

* 就是在Z=w^T*x+θ0=>Z=w^T*x+b,和b这个截距。是的y的值在-1和1之间
* 为了更加简洁的介绍支持向量机，我们需要先引入一种新的标记。考虑使用线性分类器解决“特征为x目标为y”的二元分类问题。这次，我们使用y∈{−1,1}来标记两个分类（而不是之前的y∈{0,1}），再使用参数向量w,b代替之前的参数向量θ和θ0（就是截距），于是我们现在将分类器写为：hw,b(x)=g(wTx+b)，w是参数向量，b是一个实数

**问：**为什么直接用正负一而不是0，1呢？

**答：**因为在表示几何间隔和函数间隔的时候，+1表示的是1类的间隔，而-1表示的是另一类的间隔，可以通过统一的公式来表示点到超平面的间隔


## 间隔

在讲函数间隔之前，我们先来看下一个直观的理解：

![](http://nbviewer.jupyter.org/github/zlotus/notes-LSJU-machine-learning/blob/master/resource/chapter06_image01.png)

* 如果求关于点A的预测，我们会非常确定其值为y=1。相反，点C距离判别边界很近，虽然它在判别边界y=1一侧，但是如果判别边界稍有变动，它就有可能被放在y=0一侧。因此，我们对点A预测的“信心”强于点C。而点B则在两者之间，通常来说，如果点距离判别边界越远，模型做出预测的“信心”就越强。也就是说，如果对于给定训练集，可以找到一条能够准确并可信（即样本距离边界很远）的预测所有训练样本的判别边界，则称这个拟合是良好的。

* 故现在我们从间隔的角度，或者说是信心，因为这反映出该拟合对分类结果的“信心”很足。所以，“信心”是一个很好的指标，后面我们将使用函数间隔形式化这个指标。考虑其空间意义，就是两类被模型描述相差越远（注意不是实际两类差别，因为两类差别是客观不变的，这里的差别指两类离这分界模型距离），那么模型就是最好的。在svm，用到的就是间隔（或者说是信心）。


## 函数间隔

* 先给出定义，给定一个样本，它的函数间隔是：![](http://img.blog.csdn.net/20150724151103540)因此可知，如果要得到一个值尽可能大的函数间隔，当y为-1时，这时要求![](http://img.blog.csdn.net/20150724151135069)要尽可能的小于零，相反，如果y为1时，这时要求![](http://img.blog.csdn.net/20150724151135069)要尽可能的大于零。同时，在这里，我们也可以理解下面一个问题：

* 我们发现，最终最大间隔分类用的不是函数间隔，而是几何间隔，为什么呢？因为有一个性质导致其函数间隔不能有效的反映预测的可信度，就是在函数间隔成倍增加的时候，分类超平面并没有改变，也就不能用函数间隔的大小来衡量超平面是否合理。但也许，你也有如下问题:


**问：**为什么要成倍增加来看函数间隔?

**答：**因为这里是证明函数间隔不能用来作为依据，那么只要证明存在一种情况使得函数间隔变化结果却不变就可以了（注意用简单的二维的例子）。几何间隔就是点到直线的距离，高维就是上面的几何间隔，而函数间隔就是未标准化的几何间隔。


### 几何间隔


![](http://img.blog.csdn.net/20150724151517346)

回到一开始讲间隔的时候，给出几何间隔的定义公式,定义A点的几何间隔为：![](http://img.blog.csdn.net/20150724151725371)


### 函数间隔和几何间隔的关系
* 函数间隔/||w|| =几何间隔，函数间隔会随着w和b的缩放而缩放，但是对于算法的参数选取没有意义。几何间隔不会随着w和b的缩放而缩放。
* 高维就是上面的几何间隔，而函数间隔就是未标准化的几何间隔。


**问：||w||存在的意义是什么？**

将(w,b)变为(2w,2b)相当于给函数间隔乘了系数2，于是我们发现，如果通过改变w,b的取值，我们可以让函数间隔变得很大，然而分类超平面并没有改变，所以单纯的通过这种方式改变函数间隔的大小没有什么实质意义，应该引入一种标准化条件，比如令∥w∥=1（此时是垂直于超平面的单位向量）。

>我的理解，纯碎w，b改变，函数间隔改变了，但当纯碎w，b改变时，对于这里的g(z)来说，效果没有变化，故纯粹改变w，b时，不能用来作为确定更好参数的前提。函数间隔倍数增加无效，几何间隔加减变化有效，指倍数不能对正负改变但加减可以。

>看了几遍之后，终于理解，**这里的||w||要联想低维的点到直线的距离就好**，没有那么复杂，就是二维点到直线距离的除数而已



**为什么在求解最优化的时候，函数间隔可以设为1？**

**答：按比例缩放参数(w,b)对假设结果没有任何影响，我们现在可以利用这一点。我们现在来引入限制条件：对于给定的训练集，以(w,b)为参数的函数间隔必须为1，在这里，数值是什么关系的，这只是一种约束，通过数值等比例缩放得到。**

>**也就是说函数间隔和最优解无关，因为函数间隔可以任意变化，但问题还是原来的问题**


## 有关凸优化

>因为下面会用到比较多凸优化，所以在这里先说下它的简单定义。

* 简单的说，优化问题中，目标函数为凸函数，约束变量取值于一个凸集中的优化问题称为凸优化，举个简单例子，设S为凸集，f(x)为S上凸函数，则问题min f(x) s.t. x属于S。为一个凸优化。

* 设S为n维空间中的一个点集，X1、X2为S中的任两点。若对于任给的t，0<=t<=1，点X=tX1+(1-t)X2也属于S，则称S为n维空间中的一个凸集。组合tX1+(1-t)X2称为X1和X2的凸组合。简单的说，若两点在一个点集中，那么连接这两点的线段上所有点也在这个点集中，这样的点集就称为凸集。



## svm分类

SVM包括：线性可分支持向量机、线性支持向量机、非线性支持向量机。其区分如下。

线性可分支持向量机 ：要求硬间隔最大化，也叫硬间隔支持向量机。

线性支持向量机 ：要求软间隔最大化，也叫软间隔支持向量机。

非线性支持向量机    ：包含核函数的SVM。

注：“线性支持向量机”中线性指的是最终的分割超平面是线性的，而不是指数据是线性可分的，于是这三种SVM也可以这样描述：

线性可分支持向量机 ：数据线性可分，分割面为线性。

线性支持向量机 ：数据线性不可分，分割面为线性。

非线性支持向量机    ：数据无所谓，分割面不为线性。

>下面，我们就按照这样的顺序来讲解svm的有关内容。


##  线性可分支持向量机

![](http://img.blog.csdn.net/20160719115921950)

>也就是这样的情形，下面我们通过间隔最大化得到SVM的分隔超平面，然后进行分类


<img src="{{ site.img_path }}/Machine Learning/svm_a1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

<img src="{{ site.img_path }}/Machine Learning/svm_a2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

<img src="{{ site.img_path }}/Machine Learning/lagelangrei1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>









**目标函数为凸函数，约束变量为凸集**

### 非凸性约束
* 这是一个糟糕的非凸性约束，即参数w可能位于一个单位圆环/球面上，不是凸集），这显然不是可以提交给标准的凸优化软件（如梯度下降法、牛顿法等）去解决的问题。

* 我们发现在第一个优化问题中，存在非凸性的限制条件，而在第二个优化问题中，存在非凸性的优化目标，所以我们并不能保证软件可以找到全局最小值--见讲义ch7


### 使其能够求解
* 按比例缩放参数(w,b)对假设结果没有任何影响，我们现在可以利用这一点。我们现在来引入限制条件：对于给定的训练集，以(w,b)为参数的函数间隔必须为1。
* 相当于优化目标可以类似用2次函数求解法来求解，也就是可以求解了

目的：得到最优间隔分类器的一般优化参数的公式


## 拉格朗日乘数

**问：为什么要使得αi>0？和gi(w)<0**
**答：因为他们只是任何原始约束条件，为了满足拉格朗日，要满足约束条件，即αi>0**


[简单拉格朗日](https://en.wikipedia.org/wiki/Lagrange_multiplier#Examples)

## 引入对偶优化的原因
1. 对偶问题通常更加简单求解。
2. 它将引出我们优化问题的对偶形式，然后引出通过使用核方法，将保证在我们将最优间隔分类器问题推广到极高维度空间时算法的高效求解。


**问：为什么maxL(w,α,β)在满足拉格朗日系数时=f（x）--ch7**
**答：先把w看成常数，则maxL(w,α,β)时，f(x)就可以看做是一个常数，则常数的最大值就是他自己**

**问：为什么对于原问题要先最大化关于a的函数，然后求最小化关于w，b的函数？**
**答：最小化关于w，b的函数很好理解，因为这就是我们要求的1/2||w||^2的目标，而最大化关于a的函数的原因是，根据拉格朗日算子中a，beta的约束，当要求达到时，就可以：满足最大化关于a的函数就是没有约束条件的原问题，这是在求最小化关于w，b的函数就可以直接求了，对偶函数亦然**

## 一些函数定义
1. 仿射函数/变换是指存在ai,bi使hi(w)->0约束条件
2. 进一步假设gi是严格可用的，即对于所有i存在w能够使gi<=0，即式子有意义
3. 如果存在满足KKT条件的w∗,α∗,β∗，则原始问题与对偶问题一定有解，就是可以存在最优解
4. 即约束条件gi(w∗)≤0激活”，成为一个活动约束（active constraint）并处于取等号的状态，即g(w*)=0.

###仿射函数
h（i）是仿射函数（仿射函数/变换是指存在ai,bi使得hi(w)=aTiw+bi，而“仿射变换”是指线性变换后加上截距项bi使整体平移，即线性变换是固定原点的，而仿射变换是可以平移的

### KKT
* 当原始问题和对偶问题的最优值相等：时，可以用求解对偶问题来求解原始问题（当然是对偶问题求解比直接求解原始问题简单的情况下，在这里也要注意在支持向量机中应用了其性质），但是到底满足什么样的条件才能使的呢，这就是的 KKT条件

### 活动约束
就是：gi(w∗)=0
我们将通过这个条件知道支持向量机只有一小部分“支持向量”。当讲到序列最小优化算法（SMO）时，KKT对偶互补条件也会给我们一个验证其收敛特征的方法。

**问：**怎么知道函数间隔为1时，所对应的点就是支持向量，也就是计算函数间隔的点的？
**答：**看《统计学习方法》p103，该例子中是先求出最优解，然后再求支持向量。反过来也成立

### 支持向量
也就是虚线上的三个点，只有这三个点的拉格朗日乘数不为零，也只有这三个样本的函数间隔等于1，其余样本的函数间隔都严格大于1。再多说一点，有时会有gi,αi都等于零的情况，但通常gi=0时αi是非零的，所以那些函数间隔为1的样本就是那些αi不等于零的样本），这三个样本也称为这个问题的支持向量（support vectors）。支持向量的数量通常都比训练集样本总量少很多。


### 我们使用拉格朗日对偶的原因
1. 我们可以通过解对偶优化问题来得到原始优化问题的最优值（这么做的原因是，对偶问题通常更加简单，而且与原始问题相比，对偶问题具有更多有用的性质，稍后我们会在最优间隔分类器即支持向量机问题中见到），接下来看在什么情况下此式成立。
2. 某些条件下，把原始的约束问题通过拉格朗日函数转化为无约束问题，如果原始问题求解棘手，在满足KKT的条件下用求解对偶问题来代替求解原始问题，使得问题求解更加容易。
>用对偶很重要的原因在于：在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转换为对偶问题，通过求解对偶问题而得到原始问题的解，还有就是通过svm

### 求解的过程：
1. 首先用对偶性求出a，=>w用a表示=>是a,w带入原始问题求中解b

只有拉格朗日乘数a而没有beta_，因为此问题中只含有不等式约束条件。


由偶互补约束条件求出支持向量，先求出支持向量，=>求出用支持向量下的最大间隔，即最优解

假设我们已经通过拟合训练集得到模型的参数，现在想要预测一个新的输入x，那么我们会计算w^Tx+b(表示的就是z)，当且仅当这个值大于0时，模型才会给出结论y=1

**问：为什么在向量机中拉格朗日对偶比较好求**
**问：最大间隔的限制条件是怎么给出的**



# 核方法
核方法的主要思想是基于这样一个假设：“在低维空间中不能线性分割的点集，通过转化为高维空间中的点集时，很有可能变为线性可分的” ，

任何半正定的函数都可以作为核函数。所谓半正定的函数f(xi,xj)，是指拥有训练数据集合（x1,x2,...xn)，我们定义一个矩阵的元素aij = f(xi,xj)，这个矩阵式n*n的，如果这个矩阵是半正定的，那么f(xi,xj)就称为半正定的函数。这个mercer定理不是核函数必要条件，只是一个充分条件，即还有不满足mercer定理的函数也可以是核函数。常见的核函数有高斯核，多项式核等等，在这些常见核的基础上，通过核函数的性质（如对称性等）可以进一步构造出新的核函数。SVM是目前核方法应用的经典模型。

## 低维转高维要解决的问题
一是由于是在高维度空间中计算，导致curse of dimension问题；二是非常的麻烦，每一个点都必须先转换到高维度空间，然后求取分割平面的参数等等

## 核方法和分类（或者回归）的问题
“为什么我们要关心向量的内积？”，一般地，我们可以把分类（或者回归）的问题分为两类：参数学习的形式和基于实例的学习形式。参数学习的形式就是通过一堆训练数据，把相应模型的参数给学习出来，然后训练数据就没有用了，对于新的数据，用学习出来的参数即可以得到相应的结论；而基于实例的学习（又叫基于内存的学习）则是在预测的时候也会使用训练数据，如KNN算法。而**基于实例的学习一般就需要判定两个点之间的相似程度**，一般就通过向量的内积来表达。从这里可以看出，核方法不是万能的，它一般只针对基于实例的学习。

## 核技巧
1. 讲输入空间映射到一个高维空间（一般是这样）
2. 通过內积求出相似性，这个和最大间隔求內积的道理是一样的，（內积的作用就是使求解变的简单）


这个核就是最开始提到过的会将原始空间映射为无穷维空间的那个家伙。不过，如果选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间：


**问：为什么核函数可以低维转高维**
**答：就是通过一个映射实现的，可能转化为维数极高的特征空间**

## 核方法的直观理解
能够直观（这种直观印象并非严格成立）的看出，如果向量ϕ(x)与ϕ(z)方向靠的比较近，那么根据K(x,z)=ϕ(x)Tϕ(z)可知这个值会比较大；反正，如果两个向量近乎正交（方向离的比较远），则K(x,z)=ϕ(x)Tϕ(z)将会很小。如此，我们就可以用K(x,z)度量ϕ(x)与ϕ(z)的相似度，或者x与z的相似度。

## How to select SVM kernels?
* the linear kernel works fine if your dataset if linearly separable
*  the rule of thumb is: use linear SVMs (or logistic regression) for linear problems, and nonlinear kernels such as the Radial Basis Function kernel for non-linear problems.
* it looks like both linear and RBF kernel SVM would work equally well on this dataset(linearly separable).but ，we should choose linear kernels.for the reason that Not only is it more expensive to train a RBF kernel SVM, but you also have to keep the kernel matrix around, and the projection into this "infinite" higher dimensional space were the data becomes linearly separable is more expensive as well during prediction
*  the polynomial kernel. In practice, it is less useful for efficiency (computational as well as predictive) performance reasons. 

### 核矩阵
我们已经有了一个有效核K，现在我们有一个有限集合{x(1),⋯,x(m)}（不一定是训练集，可以是任意一个包含m个点的集合），含有m个点，然后在构造一个m阶方阵K，它的第(i,j)个元素定义为Kij=K(x(i),x(j))。

##半正定矩阵
* 实对称矩阵：如果有n阶矩阵A，其各个元素都为实数，矩阵A的转置等于其本身，则称A为实对称矩阵。
* 半正定矩阵：设A是实对称矩阵。如果对任意的实非零列矩阵X有XT*A*X≥0，就称A为半正定矩阵。

## Mercer 定理：
任何半正定的函数都可以作为核函数。所谓半正定的函数f(xi,xj)，是指拥有训练数据集合（x1,x2,...xn)，我们定义一个矩阵的元素aij = f(xi,xj)，这个矩阵式n*n的，如果这个矩阵是半正定的，那么f(xi,xj)就称为半正定的函数。（这个mercer定理不是核函数必要条件，只是一个充分条件，即还有不满足mercer定理的函数也可以是核函数。）--这个有待确定。常见的核函数有高斯核，多项式核等等，在这些常见核的基础上，通过核函数的性质（如对称性等）可以进一步构造出新的核函数。

##
先默认內积为降为关键，

### 正则化

，我们都是在数据线性可分隔的假设下讨论支持向量机算法的。通过ϕϕ将数据映射到更高纬度的特征空间通常能够增加数据可分隔的概率，但是我们并不能保证总是如此。受可能存在的离群值的影响，在某些情形下，我们并不能直接用算法确定分类超平面。比如下图的情形：
为了使算法在面对线性不可分隔的数据集时有效，同时也为了降低算法对离群值的敏感度，我们重新定义了如下的优化目标

>就是把最小间隔加上一个常数，这个时候并不是求的最小的那个间隔作为支持向量，而是加上正则惩罚项后的

## 高效解决对偶问题



## 序列最小优化算法（SMO: sequential minimal optimization）

* SMO（sequential minimal optimization）算法起源于SVM, John Platt起初了一个高效解决对偶问题的方法.

最小就是指一次改变最少个αi，在这里即两个。
目的：求解最优化的a1的值

### 坐标法求最优解

考虑解决无约束最优问题：
![](http://images2015.cnblogs.com/blog/929166/201611/929166-20161114110147451-1360754235.png)svm
现在，我们只将W看做关于αi的函数，与支持向量机无关。到目前为止我们已经了解了两种优化算法：梯度下降/上升法，牛顿法，这里我们再来介绍一种新的优化算法——坐标下降/上升法（coordinate descent/ascent algorithm）：
![](http://images2015.cnblogs.com/blog/929166/201611/929166-20161114110331592-1707362330.png)

在算法里面一层循环中，我们会固定除αiαi以外的参数，然后重新优化这个关于αi的函数W。对上面的这个例子，里面的循环会按照α1,α2,⋯,αm,α1,α2,⋯的顺序进行优化。（对一些复杂的问题可能会使用别的顺序，比如，我们可以看哪个参数会带来W(α)最大幅度的增长，就选择哪个参数作为下一个优化的对象。）

而当函数W恰好符合里面一层循环可以高效运行的状态时（事实上有很多优化问题很容易固定其他参数只对一个参数求最优值，在这种情况下本算法的内层循环将会执行的非常快。而支持向量机通常也属于这种情况），坐标下降/上升法确实是一个效率很高的算法，如图所示：


![](http://nbviewer.jupyter.org/github/zlotus/notes-LSJU-machine-learning/blob/master/resource/chapter08_image03.png)

很多小伙伴看到这里其实还是有点疑惑的，感觉这样的方法一定能够求得最优解吗？我当时也有这样的疑惑，没关系，那再来看下下面的数学图像讲解：


### 二元偏导数的几何意义

![](http://h.hiphotos.baidu.com/zhidao/wh%3D600%2C800/sign=b1b9945ab8389b5038aae854b505c9e5/0df3d7ca7bcb0a467577057d6a63f6246b60af19.jpg)
比如一个椭球面,它有无数个点,有其中一点（a,b,c） 函数对x的偏导数 就是 阴影椭圆形的线框（平行于x0z面）,再建立坐标x‘0z’,仅考虑该坐标的话  有函数z=f（x）  f'（x）是z=f（x）的导数（也是斜率）同时也等于球面函数在（a,b,c）点对于x的偏导


**问：**所以到底怎么理解坐标下降/上升法？
**答：**我们可以看上图，一开始我们确定(a,b,c然后对x求偏导数，这个时候y是固定的，然后再对y求偏导数，这个时候x是固定的)，稍微想象一下就会发现最后我们是会到达局部最大/小点的！

>[坐标上升](http://blog.csdn.net/google19890102/article/details/51065297)，这个博客唯一可能会误导的是，首先要确定初始的点，而博客中没有

### 梯度下降和坐标下降的区别
* 梯度下降法又称为最速下降法，他也是下降法，不过和坐标下降法的主要区别就是多了一个下降方向的选取，在坐标下降中下降方向是沿着每一维的坐标轴方向进行的，也就是方向是类似于（0,0,1,0,0）、（0,0,0,1,0）（假设是5维）这种形式的，而梯度下降法中，下降方向变换为函数在当前点的梯度方向，当维度很高时，梯度下降的优势就要比坐标下降明显很多。



**为什么　对　１／２｜｜w｜｜^2 求偏导之后，是　w,而不是　｜｜w｜｜？**
** 1.简单的，设w=（w1,w2),x1=(1,2),y1=-1，则\\W\\^2=w1^2+w2^2,w *x1=w1+2*w2
L=1/2(||w||^2)-a1(y1(w*x1)=1/2(w1^2+w2^2)+a1(w1+2*w2)
将L对w1求偏导得到
w1+a1=0
对w2求偏导得到
w2+2a2=0
合在一起就是（w1,w2)+a1(1,2)=0，而这个不就是w-a1*y1*x1=0 **


## 关于正则化C
  是一个参数，用于控制目标函数中两项（“寻找 margin 最大的超平面”和“保证数据点偏差量最小”）之间的权重。注意，其中  是需要优化的变量（之一），而  是一个事先确定好的常量。完整地写出来是这个样子：


>最后推荐博文时间：[SVM较好的博文](http://blog.csdn.net/v_july_v/article/details/7624837)

  <!-- 多说评论框 start -->
  <div class="ds-thread" data-thread-key="2017032501" data-title=" svm" data-url=""></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"yzhhome"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共JS代码 end -->





