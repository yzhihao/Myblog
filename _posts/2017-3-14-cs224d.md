---
layout: post
title:  cs224d-word Vectors
desc: 我的博客
keywords: 'blog,Machine Learning,AI'
date: 2017-3-15T00:00:00.000Z
categories:
  - Machine Learning
tags:
  - Machine Learning
  - AI
icon: fa-book
---


## 目录
**欢迎在文章下方评论，建议用电脑看**

* 目录
{:toc}

# cs224d-word Vectors

## word2vec and Glove

下面是有关GloVe模型的描述：
<img src="{{ site.img_path }}/Machine Learning/GloVe.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>



下面是关于word2vec和Glove两个模型的区别，文章摘自[quora](https://www.quora.com/How-is-GloVe-different-from-word2vec)

<img src="{{ site.img_path }}/Machine Learning/word2vec_glove.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

>简单说明，在我看在word2vec就是一个**预测模型，**然后是可能根据上下文来预测中心词或者反过来，而Glove是说明一个词袋的词的关系，是一个**Count-based models **一开始是高维的，然后提取其中的主要因子，然后用低维来表示高维中绝大部分的信息。
>还有就是 The additional benefits of GloVe over word2vec is that it is easier to parallelize the implementation which means it's easier to train over more data, which, with these models, is always A Good Thing.，这段话说明了在大量数据的时候，GloVe比较好的并行实现！

最后，贴一张ppt，讲述Count based模型和direct predicDon模型的区别。

<img src="{{ site.img_path }}/Machine Learning/Count_based_and predicDon.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

## word2vec

因为在word2vec中，我们知道是有**两个向量来表示一个词的，分别是中心词和”旁边“词，但在最后我们只需要一个词向量来代表一个词，**下面就说明了怎么来合并这两个词！

<img src="{{ site.img_path }}/Machine Learning/word_vectors1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

下面是介绍词向量的评估，分为两种，一个是内部一个是外部评估：

<img src="{{ site.img_path }}/Machine Learning/evaluate_word.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

### 交叉熵

<img src="{{ site.img_path }}/Machine Learning/Cross_entropy1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

<img src="{{ site.img_path }}/Machine Learning/Cross_entropy2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

>这里很明白的说明了最小化交叉熵就是在最小化kl散度，好的，一开始并不知道它是什么，下面摘自网上，对于kl散度和交叉熵的解释：

<img src="{{ site.img_path }}/Machine Learning/kl_sandu.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

比如TD-IDF算法就可以理解为相对熵的应用：词频在整个语料库的分布与词频在具体文档中分布之间的差异性。交叉熵可在神经网络(机器学习)中作为损失函数，**p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。**交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。

PS：

1. 通常“相对熵”也可称为“交叉熵”，因为真实分布p是固定的，D(p||q)由H(p,q)决定。当然也有特殊情况，彼时2者须区别对待。
2. 尽管KL散度从直观上是个度量或距离函数，但它并不是一个真正的度量或者距离，因为它不具有对称性，即D(p||q)！=D(q||p)



### 词向量的再训练

下面课程用一个很简单的额例子说明了在数据量较小的情况下，去训练词向量会使得分类失去泛华能力，这是不可取的！

<img src="{{ site.img_path }}/Machine Learning/Losing_generalization_smell_vec1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

<img src="{{ site.img_path }}/Machine Learning/Losing_generalization_smell_vec2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

<img src="{{ site.img_path }}/Machine Learning/Losing_generalization_smell_vec3.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

  <!-- 多说评论框 start -->
  <div class="ds-thread" data-thread-key="2017031501" data-title="cs224d-word Vectors" data-url=""></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"yzhhome"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共JS代码 end -->


