---
layout: post
title: 线性回归
desc: 我的博客
keywords: 'blog,Machine Learning,AI'
date: 2017-2-16T00:00:00.000Z
categories:
  - Machine Learning
tags:
  - Machine Learning
  - AI
icon: fa-book
---


## 目录
**欢迎在文章下方评论，建议用电脑看**

* 目录
{:toc}

# 线性回归

## 简介
最简单的一种机器学习模型，这个其实在高中（还是初中？）的时候我们就已经学过了，只不过那个时候觉的没什么用，也就大多数还给老师了！
先来一个例子，来来简单的说说它！
假设我们有一个经调查得到的数据集，内容是关于俄勒冈州波特兰地区47间出租公寓的面积与价格之间的关系表：

<img src="{{ site.img_path }}/Machine Learning/linear_R1.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>




有了这样的数据，我们怎样才能预测波特兰地区其他出租公寓的价格，比如推如果一开始选的点就是最低点，那随机梯度下降将会怎么工作 答：会保持选的点就是最佳参数，这是因为那个偏导数为0（联系2次函数）
导出一个从面积得出价格的函数。这就转化为一个回归的问题：利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。

>通俗的说就是，给定一定量的现有数据，用这些数据来拟合出一个函数，然后对新的数据进行拟合，就可以得出一个函数值，这个值就作为预测值。

下面我们结合上面的例子，先画出其数据的点
![](https://raw.githubusercontent.com/yzhihao/notes-LSJU-machine-learning/master/resource/chapter02_image01.png)



然后我们用这些数据来拟合一条直线，然后如果给我们一个新的数据，只知道他多少平方米，这个时候我们代入那个函数h（x）=θ*x+b(θ，b是参数，x是自变量)，得到预测值，就可以作为这个房子的预测价格了！（线性回归就是这样，看！多简单是吧！）

>当然，整体上很简单，但细节我们还是要抠的，下面我们讲下细节和他的推广！

## 一般线性回归

上面我们讨论了单个属性的回归，现在我们来讨论多个属性（n个）属性的回归。
下面给出公式：

<img src="{{ site.img_path }}/Machine Learning/linear_R2.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>



其实也是同样简单，我们只需要按照最简单的1个属性那样理解它就可以了，反正我是这样干的！现在有了目标函数，我们的目标是找到能预测最准的函数，那该怎么找能，就引出下下文：

## 寻找最优模型

我们这样想，有了上面的目标函数，我们最关键的是求出那些参数是吧，那现在我们能不能转变一个方向，看看从另一个方式，得到那些最优的参数，先看个式子：

<img src="{{ site.img_path }}/Machine Learning/linear_R3.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>



这个叫成本函数，具体其他成本函数讲解可以看我的[上一篇博文](https://yzhihao.github.io/machine%20learning/2017/01/18/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7-%E4%B8%80.html),好了，为什么，肯定很多人跟我一样，一眼看到这个式子的就想问为什么？为什么是平方差？为什么是误差平方和而不是绝对值或其他损失函数？为什么前面要加个1/2？下面一个一个解释:

### 最小二乘：

首先解释一下，上面那个成本函数是最小二乘回归模型中常见的最小二乘成本函数,最小二乘又是什么？
最小二乘（LS）问题是这样一类优化问题，目标函数是若干项的平方和，每一项具有形式，具体形式如下：![](https://www.zhihu.com/equation?tex=f%28x%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%7B%5Cleft%28+a_%7Bi%7D%5E%7BT%7Dx-b_%7Bi%7D+%5Cright%29%5E%7B2%7D+%7D+)
或者如下：
![](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%5Cleft%28y_%7Bi%7D-%5Chat%7Bf%7D+%28x_%7Bi%7D%2C%5Ctheta+%29%5Cright%29%5E%7B2%7D)
其中y是真值，
![](https://www.zhihu.com/equation?tex=%5Chat%7Bf%7D%28x%2C%5Ctheta+%29)
是估计值，式1和式2是一样的，只是用的符号不同，式1中的x对应式2中的θ ，即优化中要求的变量。
说下为什么在这里我们要为前面添加一个1/2，因为在之后的求导中能更简单的化简而且不会对结果产生影响！

最小二乘法的二乘是什么:简单地说,最小二乘的思想就是要使得观测点和估计点的距离的平方和达到最小.这里的“二乘”指的是用平方来度量观测点与估计点的远近（在古汉语中“平方”称为“二乘”）,“最小”指的是参数的估计值要保证各个观测点与估计点的距离的平方和达到最小..
为什么要二乘:因为观测点和估计点之差可正可负,简单求和可能将很大的误差抵消掉,只有平方和才能反映二者在总体上的接近程度.

### 为什么要用最小二乘

为什么使用最小二乘成本函数J？这样合理吗？有那么多别的指标，比如预测值与实际值之差的绝对值、四次方等，为什么偏偏选择差的平方作为优化指标？我们将从一系列基于概率的假设中推出最小二乘回归的合理性。

首先我们得复习一下线性回归的模型及假设：

<img src="{{ site.img_path }}/Machine Learning/linear_R4.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

ε(i) ∼ N(0, σ2)，随机误差ε服从正态分布（高斯分布）
ε(i) are distributed IID，随机误差ε是独立同分布的
于是可以获得目标变量的条件概率分布：

<img src="{{ site.img_path }}/Machine Learning/linear_R5.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


整个训练集的似然函数，与对数似然函数分别为：

<img src="{{ site.img_path }}/Machine Learning/linear_R6.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


因此，最大化对数似然函数，也就相当于最小化,于是，我们发现，圈着的这个式子正是成本函数J(θ)的定义式。

综上，可以看出，在关于训练数据的概率假设中，最小二乘回归与θθ的最大似然估计一致。也正是因为最小二乘回归其实是再做最大似然估计，所以我们才会强调最小二乘是一种“很自然”的方法。（不过，概率假设并不是证明“最小二乘是一种非常易用的、合理的求解过程”这一结论的必要条件，这只是众多假设中的一种，最小二乘在这种假设下合理，除此之外还有其他假设也可以证明这一结论。）


## 求解最小二乘
有了成本函数，我们下一步自然要求解最小的成本函数，找到最优的参数，所以这里就转变成为了求最小值的问题。在这里我就介绍两种方法吧，至于牛顿法，拟牛顿法，坐标法和他们的推广就下次再讲吧！

### 正规方程组的求法
这个很简单。就是我们平时学的求解一个函数的通用方法，直接求导，然后使得导数等于0，求得驻点，最后得到最小值！对于上面的成本函数的求解如下：

<img src="{{ site.img_path }}/Machine Learning/linear_R7.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

得到结果是：

<img src="{{ site.img_path }}/Machine Learning/linear_R8.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


中间的求解过程有点点复杂。我们主要要记住那个结论的式子。然后改怎么记呢？这里给出一个小小的tips（我也是网上看到的），这个结果有个简单的记法(可以这样记，但一旦有人问这个结果的来历，可不能这样回答)：Xθ= y  =>  X^T*Xθ= X^T*y  =>  θ= (X^T X)^ -1*X^Ty

讲到正规方程组，然后再讲讲伪拟和岭回归!

### 广义逆矩阵(伪逆):

什么是广义逆矩阵，请看下图：
![]()

## 梯度下降算法

介绍完上面的正规方程组，下面来看下另一种比较简单的优化算法--梯度下降，这个算法很重要，在机器学习领域，很多地方都用到，包括那些现在比较火的深度神经网络的求解！

首先看下式子：

<img src="{{ site.img_path }}/Machine Learning/linear_R9.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>
[a表示的是步长或者说是学习率（learning rate）]

好了，怎么理解？在直观上，我们可以这样理解，看下图，一开始的时候我们随机站在一个点，把他看成一座山，每一步，我们都以下降最多的路线来下山，那么，在这个过程中我们到达山底（最优点）是最快的，而上面的a，它决定了我们“向下山走”时每一步的大小，过小的话收敛太慢，过大的话可能错过最小值）。这是一种很自然的算法，每一步总是寻找使J下降最“陡”的方向（就像找最快下山的路一样）。

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/bn9SyaDIEeav5QpTGIv-Pg_0d06dca3d225f3de8b5a4a7e92254153_Screenshot-2016-11-01-23.48.26.png?expiry=1485561600000&hmac=zP0cqdiyEZTU2CAHneHMKxY5MLTI0wnT0U_CUuniLVE)

当然了，我们直观上理解了之后，接下来肯定是从数学的角度，我们可以这样想，先想在低维的时候，比如二维，我们要找到最小值，其实可以是这样的方法，具体化到1元函数中时，梯度方向首先是沿着曲线的切线的，然后取切线向上增长的方向为梯度方向，2元或者多元函数中，梯度向量为函数值f对每个变量的导数，该向量的方向就是梯度的方向，当然向量的大小也就是梯度的大小。现在假设我们要求函数的最值，采用梯度下降法，结合如图所示：

<img src="{{ site.img_path }}/Machine Learning/linear_R10.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


下面是两种梯度下降的方式:

1. batch gradient descent 批量梯度下降: 
<img src="{{ site.img_path }}/Machine Learning/linear_R11.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


2. stochastic gradient descent (incremental gradient descent) 随机梯度下降
<img src="{{ site.img_path }}/Machine Learning/linear_R12.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


### 岭回归

<img src="{{ site.img_path }}/Machine Learning/linghuigui.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


### 随机梯度下降

* 值得注意的是，随机梯度下降可能永远都不会收敛于最小值点，参数θ将在J(θ)最小值附近持续摆动。不过，在实践中，最小值附近的解通常都足够接近最小值。另外，在随机梯度下降的实际操作中，随着迭代步骤的进行，我们会慢慢减小α的值至0，这样也可以保证参数收敛于全局最小值，而不是在其附近持续摆动

**问1：如果一开始选的点就是最低点，那随机梯度下降将会怎么工作**

**答1：会保持选的点就是最佳参数，这是因为那个偏导数为0（联系2次函数）**

**问2：梯度下降的公式是怎么来的**

**答2：理解在2次函数的时候来理解，就是参数减去导数的值，然后更新参数，来自：coursera机器学习**

**问3：批量和随机梯度的本质区别是什么**

**答4：就是成本函数的不同，批量是计算把所有的数据都误差和，然后求导得，随机的成本函数是随机拿一条数据来作为误差，然后求导**

## 局部加权回归
* 通常情况下的线性拟合不能很好地预测所有的值，因为它容易导致欠拟合（under fitting），比如数据集是一个钟形的曲线。而多项式拟合能拟合所有数据，但是在预测新样本的时候又会变得很糟糕，因为它导致数据的过拟合（overfitting），不符合数据真实的模型。
* 线性回归的推广，就是在求参的时候不同，可以得到多个”子模型“，但模型同样是线性的。

首先在局部加权回归中，上面的损失函数变为
<img src="{{ site.img_path }}/Machine Learning/linear_R13.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


其中:

<img src="{{ site.img_path }}/Machine Learning/linear_R14.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


函数中的τ称作带宽（bandwidth）（或波长）参数，它控制了权值随距离下降的速率。如果τ取值较小，则会得到一个较窄的钟形曲线，这意味着离给定查询点x较远的训练样本x(i)的权值（对查询点xx附近训练样本的拟合的影响）将下降的非常快；而τ较大时，则会得到一个较为平缓的曲线，于是查询点附近的训练样本的权重随距离而下降的速度就会相对比较慢。


若直接求解得到最终的参数求解式子是：

<img src="{{ site.img_path }}/Machine Learning/linear_R15.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


下面从图来直观感受局部加权回归和普通回归的不同：

1. 普通回归

<img src="{{ site.img_path }}/Machine Learning/linear_R16.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


2. 局部加权回归

<img src="{{ site.img_path }}/Machine Learning/linear_R17.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>




## 缩减特征
* 求解的时候都要做特征缩减
* 为了使得随机梯度下降工作的更好，一般要把轮廓变为差不多圆形，就要用到特征缩放。
* 方法：就是(x-（均值）)/（最大到最小的范围）
* [具体看课件](https://www.coursera.org/learn/machine-learning/supplement/CTA0D/gradient-descent-in-practice-i-feature-scaling)
* si is the range of values (max - min), or si is the standard deviation.除数可以是标准差，也可以是极差
* 这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。




### 方向导数
自变量是多个标量，或者理解成一个多维的向量。那么，函数随自变量的变化怎么刻画呢？一个方法，就是衡量函数在给定方向上的变化率，这就是方向导数。方向导数的特例，就是函数随各个自变量（标量）的变化率，即函数的偏导数，也就是函数沿各个坐标轴正方向的方向导数。

[理解方向导数和](https://www.zhihu.com/question/36301367)

### 梯度的理解
* 在个偏导数的方向就是其梯度的方向，这也是变化最快的方向。


看高数下册
[详数学推导](http://blog.csdn.net/lotus___/article/details/20546259#comments)
最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。

参考资料：


《统计学习方法》-李航<br>
《机器学习》-周志华<br>
《机器学习实战》-Peter Harrington<br>
斯坦福大学公开课-机器学习<br>
网上的各位大牛的博文<br>

  <!-- 多说评论框 start -->
  <div class="ds-thread" data-thread-key="201702161" data-title="linear_R" data-url=""></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"yzhhome"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共JS代码 end -->




