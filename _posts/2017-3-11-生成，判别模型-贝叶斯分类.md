---
layout: post
title: 生成模型，判别模型，贝叶斯算法
desc: 我的博客
keywords: 'blog,Machine Learning,AI'
date: 2017-3-12T00:00:00.000Z
categories:
  - Machine Learning
tags:
  - Machine Learning
  - AI
icon: fa-book
---


## 目录
**欢迎在文章下方评论，建议用电脑看**

* 目录
{:toc}


## 生成模型，判别模型，贝叶斯算法

* 生成方法的特点：上面说到，**生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布P(Y|X)，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。**

* 判别方法的特点：判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X)。不能反映训练数据本身的特性。但**它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。**由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

* 需要注意的是，在分类算法中，**大部分都是判别模型。因为如果使用生成模型，那么，我们就需要去对p(x)建模，但这增加了我们的工作量**，这让我们很不爽（除了上面说的那个估计得到P(X)可能不太准确外）。实际上，**因为数据的稀疏性，导致我们都是被强迫地使用弱独立性假设去对p(x)建模的，所以就产生了局限性**。所以我们更趋向于直观的使用判别模型去分类。

举个例子，贝叶斯就是一种生成模型算法，生成学习算法：GLA首先确定p(x|y)和p(y)，由贝叶斯准则得到后验分布![](http://images.cnitblog.com/blog/405927/201412/050156335615212.png),通过最大后验准则进行预测，![](http://images.cnitblog.com/blog/405927/201412/050156342172611.png)
对y进行分类，建立多个模型，并求其概率。

>[这篇博文](http://blog.csdn.net/zouxy09/article/details/8195017)详细讲述了生成模型和判别模型的一些联系和区别，值得一看。


# 贝叶斯

* 贝叶斯定理是一种“根据数据集内容的变化而更新假设概率”的方法。
* ps：上面引号中的内容用另一种方式表达就是：假设的概率随看到的数据而变化。

于是对于事件A和B，贝叶斯定理的表达式可写成：

<img src="{{ site.img_path }}/Machine Learning/byesi.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

P(A|B) = P(A)P(B|A) / P(B)在这种解释里，每项的意义如下：
    1. P(A)：先验概率。即：在的得到新数据前某一假设的概率。
    2. P(A|B)：后验概率。即：在看到新数据后，要计算的该假设的概率。
    3. P(B|A)：似然度。即：在该假设下，得到这一数据的概率。
    4. P(B)：标准化常亮。即：在任何假设下得到这一数据的概率。

    先验概率P(A)：取出饼干的碗是碗1的概率。结果是1/2。
    后验概率P(A|B)：得到的是香草饼干，且该饼干从碗1中拿到。待求。
    似然度P(B|A：在碗1中得到香草饼干的概率。结果是3/4。
    标准化常量P(B)：饼干是香草饼干的概率。结果是5/8。

>在概率论中，我们就学过，其实贝叶斯就是结果推向过程的一种方法，它的本质还是条件概率



**问：为什么可以去掉贝叶斯的分母？**
**答：因为那个表示的总概率，而标准化常量P（B）都是相同的，它是一个标准化常量，可以忽略**

#### 朴素贝叶斯假设

1. 设x(i)对于给定的条件y是独立的，这个假设叫做朴素贝叶斯假设，即一个词是在邮件中出现的事件和其他词是在邮件中出现事件是独立的。

>显然为假，所以称朴素贝叶斯

**注意：**xi....x3,x4,⋅xn，不同的邮件n是相同的，指的是字典单词的总个数。

### 拉普拉斯平滑（Laplace smoothing）
目的：**改进朴素贝叶斯算法的不足:避免分子出现为0的情况**

下面出现的符号还是以上面垃圾邮件的例子为准。
p(x1| c1)是指的:在垃圾邮件c1 这个类别中，单词x1出现的概率。（x1 是待考察的邮件中的某个单词）
定义符号：
   1. n1 ：在所有垃圾邮件中单词x1 出现的次数。如果x1 没有出现过，则n1 = 0。
   2. n：属于c1 类的所有文档的出现过的单词总数目。

得到公式
   	`p(x1|c1)= n1 / n`
而拉普拉斯平滑就是将上式修改为：
  ` p(x1|c1)= (n1 + 1) / (n + N)`
   `p(x2|c1)= (n2 + 1) / (n + N)`
   `......`

其中，N是所有单词的数目。修正分母是为了保证概率和为1。

举个例子：中国男足vs韩国男足的前5场的比分是0:5，那预测第六场中国队胜出的概率是多少时难道给0/5，这绝壁不行。所以分子分母都加1，变成1/6。

>由于朴素贝叶斯网络用的不多，现在还不打打算讲它。以后有机会在讲吧！



参考资料：

《深度学习》-Bengio<br>
《统计学习方法》-李航<br>
《机器学习》-周志华<br>
《机器学习实战》-Peter Harrington<br>
斯坦福大学公开课-机器学习<br>
网上的各位大牛的博文<br>

  <!-- 多说评论框 start -->
  <div class="ds-thread" data-thread-key="201701181" data-title="feature engineering" data-url=""></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"yzhhome"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共JS代码 end -->