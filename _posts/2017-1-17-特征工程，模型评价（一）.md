---
layout: post
title: 特征选择，模型评价
desc: 我的博客系统介绍
keywords: 'blog,Machine Learning,AI'
date: 2017-1-18T00:00:00.000Z
categories:
  - Machine Learning
tags:
  - Machine Learning
  - AI
icon: fa-book
---

## 目录
**欢迎在文章下方评论，建议用电脑看**

* 目录
{:toc}

# 写在前面

	在大二上学期，从javaweb转到机器学习，很久没有发博文了，我想说不是因为我懒，而是因为我真的发不了，没有能力发，从大二12月到现在，经过一段时间的学习，现在，我想把自己学过的笔记，结合自己的理解，尽量从最简单的角度看这些咋一看很复杂的机器学习算法。会写成一个系列！肯定错误会很多，希望大神们谅解我这个小白（逃），也希望可以通过写博文，而不是简单的学习笔记，可以加深自己对机器学习的印象和理解！
	下面开始吧！
	机器学习，先贴下知乎的几句话，感觉特别赞同：
	机器学习是什么？给你一堆数据，你猜一下大概是怎么一回事，交给电脑让它算算看看是不是像这么一回事。那么为什么叫这个名字，不叫“计算机协助下的统计学假设检验与分布函数搜索”？因为听起来酷炫，好骗钱。学了机器学习，才知道，名字可以起的如此装逼。。。
	下面开始吧！第一篇

# 特征工程，模型评价与选择（一）

## 获取数据

* 这部分途径有很多很多，在这篇博文不讲，下次有机会我会讲下我学习网络爬虫来获取数据的学习经历！

##  数据清洗（Data Cleaning）
* 当有了数据，我们可以统计一下各个变量的缺失值情况
* 然后对缺失部分进行处理，如果是连续变量，可以采用预测模型，例如 Age，可以找到类似的数据群体，然后取最多的，或者最多群体的平均值。


## 特征工程（Feature Engineering）
* 特征工程就是选择一些表示典型特征的数据，来替代原始数据作为模型的输入，进而得到比较好的输出效果。
* 连续数据做一下归一化，即把大范围变化的数据范围缩小至 0～1 或者 －1～1 之间。然后把不相关的变量 （丢）drop 掉。

>因为一个老是说概念可能会不那么清晰，在这篇文章中有例子详解[推荐系统例子](http://blog.jobbole.com/74951/)

>还有这篇博文[特征工程怎么做](http://www.jianshu.com/p/35135ab0a627)也值得借鉴

下面直接上图，在大体上对特征工程有个大概了解！

<div>
<img src="{{ site.img_path }}/Machine Learning/feature_engineering1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>
</div>


### 特征提取与特征选择


* **特征提取：**特征选择也叫特征子集选择 ( FSS , Feature Subset Selection ) 。是指从已有的M个特征(Feature)中选择N个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程,是提高学习算法性能的一个重要手段,也是模式识别中关键的数据预处理步骤。对于一个学习算法来说,好的学习样本是训练模型的关键。.
* **特征选择：**特征提取是计算机视觉和图像处理中的一个概念。它指的是使用计算机提取图像信息，决定每个图像的点是否属于一个图像特征。特征提取的结果是把图像上的点分为不同的子集，这些子集往往属于孤立的点、连续的曲线或者连续的区域。

>提取就是通过降维技术来抽出主要信息//是计算机视觉的概念，通过映射实现，可以理解为高维的映射到低维，不删除具体特征，而是舍去那些通过映射后不能代表原来主要信息的数据
>选择就要舍去一些特征，拿到整体主要特征//要删除一些特征

二者是直接关联的关系.<br>

　　一般认为, 特征选择是指在拿到一堆原始数据的时候，选取有用的feature，以备进行机器学习使用。
　　比如你拿到的是文本信息，那么里面的单词就是原始数据了， 那么你要考虑的是，数字保留不保留？大小写要不要区分？一些常用词比如of, on, by之类的要不要扔掉？等等。对于图像处理和视频处理也有一套相应的规范和实践。通过下面即将要讲的方法，就可以知道怎么来选择特征和舍去特征。<br>
　　而特征提取（extract），比如咋做图像识别的时候，由于一个像素表示一个特征的话，现在的照片随随便便就1024*1024像素的，这时候我们就要通过特征提取的方法来抽出最能代表这张照片的主要信息。特征提取（extract）的方法有很多，主要有PCA,PCA的SVD,ICA,还有因子分析（Fachor Analysis）这些讲起来会比较复杂，所以下次要独立一篇再讲一遍（其实也是我现在还没有很好的掌握这些方法-逃。。。）


### 简单实践
　　讲到特征选择，最近有看下天池的一个比赛:[Repeat Buyers Prediction](https://tianchi.shuju.aliyun.com/getStart/introduction.htm?spm=5176.100066.333.1.1Ar5Ir&raceId=231576)，也是现在大三大佬们的大数据作业，任务就是根据用户3个月在天猫的行为日志，建立用户的品牌偏好，并预测他们在将来一个月内对品牌下商品的购买行为。在作业中老师给的特征只是用户之前的行为，所以最终效果真的有点差，百分之五点多的准确率，百分之五点多的召回率，百分之五点多的f1得分，嗯，我纯靠猜都可能比它强吧。。。<br>
　　所以我去下载了下官网的数据，做了下数据的简单可视化，在用户的年龄段（age range）和性别（gender）方面是很大影响的，具体图如下[]()
然后还有就是商品的类别和品牌也应该是有有影响的（这个想想就能理解了吧），但由于给我的数据量太大了，本来想做可视化折线图的，运行他就，效果也不明显也就放弃了。<br>
　　具体经过特征添加后的最终结果，我想应该会在**回归**那篇讲讲（之后写）

### 卡方检验
* 卡方值和卡方检验其实在我们高中的时候就学过了，想想是吧！但是那个时候觉得它并没有什么卵用，也就早“还给老师”。其实也很简单，**卡方值检验就是检验A,B事物之间的相关程度**
* 先直接推荐一篇博文，结合例子，很好理解：[卡方检验](http://www.blogjava.net/zhenandaci/archive/2008/08/31/225966.html)

>在我的项目中，有用到卡方检验来提取特征词，然后来减少无关特征，达到特征选择的目的，实践证明，确实好用！

### 信息量
* 首先先说下信息量的问题，一条信息的信息量跟这个信息能解答的问题的不确定性有关。能解答的问题越不确定，这条信息的信息量越大，也就是说这条信息的熵越大。--咦，说好的信息量，怎么又来了个信息熵的呢？好吧，说说信息熵
* 信息熵也叫香农熵（信息论之父——克劳德·香农，在 1948 年提出“ 信息熵解决了信息的度量问题），就是用来衡量信息量的大小。熵这个字出自与热力学，表示系统混乱的程度，在信息论中我们用信息熵来表示信息的大小。简单理解信息的不确定性越大，信息熵就越大，信息的不确定性越小，信息熵也就越小。

　　嗯，有点抽象，那就来结合例子来理解：--以下例子来自《数学之美》（是本好书）。

　　假设我错过了某年的世界杯比赛，现在要去问一个知道比赛结果的朋友“哪支球队最终获得世界杯冠军”？他要求我猜，猜完会告诉我是对还是错，但我每猜一次就要给他一块钱。那么我需要付给他多少钱才能知道谁是冠军？我可以把球队编号，从1到32，然后问“冠军的球队在1-16号中吗？”。假如他告诉我对了，我就问“冠军的球队在1-8号中吗？”。如果他告诉我不对，我就自然就知道冠军队在9-16号中。这样我只需要猜5次就可以知道哪支球队是冠军了。所以，“谁是世界杯冠军”这个问题的答案的信息量只值5块钱。<br>
  香农用“比特”（bit）来作为信息量的单位。像上边“谁是世界杯冠军”这个问题的答案的信息量是5比特。如果是64支球队，“谁是世界杯冠军”这个问题的答案的信息量就是6比特，因为我还要多猜一次。<br>
　　对足球了解的朋友看到这有疑问了，他觉得他不需要5次来猜。因为他知道巴西，西班牙，德国等这些强队夺冠的可能性比日本，韩国等球队大的多。所以他可以先把强队分成一组，剩下的其它队伍一组。然后问冠军是否在夺冠热门组里边。重复这样的过程，根据夺冠的概率对剩下的候选球队分组，直至找到冠军队。这样也许三次或四此就猜出结果了。因此，当每支球队夺冠的可能性（概率）不一样时，“谁是世界杯冠军”这个问题的答案的信息量比5比特少。<br>

　　香农指出，“谁是世界杯冠军”这个问题的答案的信息量是：<br>
**H = -(p1*log(p1) + p2 * log(p2) + ... + p32 * log(32))**, 其中log是以2为底数的对数，以下本文中的log都是以2为底的对数，下边不再特别说明。这就是衡量信息量多少的公式，它的单位是比特。之所以称为熵是因为它的定义形式和热力学的熵有很大的相似性。对于一个随机变量X的信息熵的定义公式为：<br>
**H(X)＝-∑P(xi)logP(xi)**，其中xi是随机变量X的可能取值。

>应该比较好懂了吧，我就是这样看懂的

信息熵计算：

<img src="{{ site.img_path }}/Machine Learning/feature_engineering2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>
</div>

信息熵图：


<img src="{{ site.img_path }}/Machine Learning/feature_engineering3.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

可以看到，在p=0.5（意味着在二元中概率事件发生概率相等的情况下）的时候熵值是最大的，也就是说这个时候是是确定性最低的，最混乱的。（延伸到在足球的例子中的意思就是每个队的获胜概率都相等的时候，那个时候信息量（熵）最大，你就越难猜那个队会赢）

### 信息增益
* 说完信息熵，之后我们来说说信息增益，但说信息增益之前，我们说下条件熵
* 条件熵表示在已知第二个随机变量 X 下第一个随机变量 Y 信息熵的大小。条件上用 **H(Y|X)** 表示

<img src="{{ site.img_path }}/Machine Learning/feature_engineering4.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

咦，看到上面式子就想起条件概率，其实，条件熵可以类比条件概率，他表示：在随机变量 X 的基础上我们引入随机变量 Y，假设 Y 和 X 有一定的关系。那么 Y 的信息熵会相对减小。

>条件熵还是很好理解的吧！不理解吗？还是结合足球的例子，比如你知道其中一个队是铁定不会赢的了，那你要现在要猜那个队会赢，那个难度就下降了是吧，因为信息熵下降了！

知道条件熵之后，信息增益就很简单了，他就是：g(X,Y) = H(X) – H(X||Y)，表示的就是条件熵和原来熵的差值

>嗯，概念就讲到这里，之后我会写决策树的博文，到时还会复习一下信息熵。介绍一下信息增益比的概念

### 互信息
* 测量训练数据中xi与y的相关度。该算法可能会使得我们选择的都是与标签y强相关的特征值。在实践中，我们通常选择能够表示xi与y间的互信息（mutual information）
* 互信息(Mutual Information)是度量两个事件集合之间的相关性(mutual dependence)。意义就是：由于事件A发生与事件B发生相关联而提供的信息量。
* 数学公式：

<img src="{{ site.img_path }}/Machine Learning/feature_engineering5.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

应该不难理解，但这里有一个问题：<br>
　　**问：**《数学之美》 上面 互信息 的公式是：I(X;Y)=H(X)-H(X|Y)；又看到 《统计学习方法》 上有一个 信息增益 的公式：G(D,A)=H(D)-H(D|A)。这不是一样吗？难道互信息就是信息增益？<br>
　　**答：**IG（信息增益）有两种定义。。一般IG是指KL散度（相对熵）
但在Desicion Tree（决策树）的IG一般是指KL散度的期望，然后正好就是互信息了，其实我想就简单理解并记住计算互信息的公式，然后在机器学习中信息增益是一种特殊的情况，就是她是KL散度的期望。
先看p对q的相对熵为
<img src="{{ site.img_path }}/Machine Learning/feature_engineering6.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

然后是KL散度的期望=互信息<br>

<img src="{{ site.img_path }}/Machine Learning/feature_engineering7.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


**总结：**其实信息论要讲起来是一门挺深的课。而我是想用最简单的方式理解最多的知识，可能讲的比较粗略，有兴趣的小伙伴可以看看信息论的书！
最后，用一张图结束:


<img src="{{ site.img_path }}/Machine Learning/feature_engineering8.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>



## 模型评价

### 模型指标

1. 正确率 = 提取出的正确信息条数 /  提取出的信息条数
2. 召回率 = 提取出的正确信息条数 /  样本中的信息条数

>两者取值在0和1之间，数值越接近1，查准率或查全率就越高。

3. F值  = 正确率 * 召回率 * 2 / (正确率 + 召回率) （F 值即为正确率和召回率的调和平均值）

不妨举这样一个例子：某池塘有1400条鲤鱼，300只虾，300只鳖。现在以捕鲤鱼为目的。撒一大网，逮着了700条鲤鱼，200只虾，100只鳖。那么，这些指标分别如下：

    正确率 = 700 / (700 + 200 + 100) = 70%
    召回率 = 700 / 1400 = 50%
    F值 = 70% * 50% * 2 / (70% + 50%) = 58.3%

不妨看看如果把池子里的所有的鲤鱼、虾和鳖都一网打尽，这些指标又有何变化：<br>

    正确率 = 1400 / (1400 + 300 + 300) = 70%
    召回率 = 1400 / 1400 = 100%
    F值 = 70% * 100% * 2 / (70% + 100%) = 82.35%

　　由此可见，正确率是评估捕获的成果中目标成果所占得比例；召回率，顾名思义，就是从关注领域中，召回目标类别的比例；而F值，则是综合这二者指标的评估指标，用于综合反映整体的指标。<br>

>关于模型评价的指标，还有很多，比如ROC曲线和AUC，敏感代价曲线，比较检验，下次再讲吧，这里讲简单点的

## 模型选择

### 过拟合和欠拟合
**过拟合（overfitting）：学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了。
欠拟合（underfitting）：学习能太差，训练样本的一般性质尚未学好。**

在周志华教授的《机器学习》中有个比较的类比：

<img src="{{ site.img_path }}/Machine Learning/feature_engineering9.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

下面在那一个真实的例子：
如果我们有6个数据，我们选择用怎么样的回归曲线对它拟合呢？看下图

<img src="{{ site.img_path }}/Machine Learning/feature_engineering10.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

1. 可以发现得到的直线y=b+a*x 并不能较为准确的描述训练数据的形态，我们说这不是一个良好的拟合。这也叫做欠拟合
2. 如果我们再加入一个特征值 x^2，得到y=a+b*x+c *x^2于是我们得到一个稍好的拟合
3. 最后我们直接用五阶多项式去拟合，发现对于训练样本可以很好的拟合，但是这样的模型对预测往往效果不是非常好，这叫做过拟合（overfitting）。

在这里我们可以发现，原来过拟合和欠拟合和模型复杂度是相关的，具体描述如下图


<img src="{{ site.img_path }}/Machine Learning/feature_engineering11.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

>好了，应该了解什么叫欠拟合和过拟合了吧！


当然，为了防止过拟合，也会有cross validation，正则话，dropout等等方法，以后会一一介绍。



### 偏差/方差权衡

　　偏差-方差分解是解释学习器泛化性能的重要工具。在学习算法中，偏差指的是预测的期望值与真实值的偏差，方差则是每一次预测值与预测值得期望之间的差均方，当然还有任务本身的噪音。实际上，偏差体现了学习器预测的准确度，而方差体现了学习器预测的稳定性。噪声表达当前任务在任何学习算法所能达到的泛化误差的下界，通过对泛化误差的进行分解。可以得到：

* 期望泛化误差=方差+偏差+噪声
* 偏差刻画学习器的拟合能力
* 方差体现学习器的稳定性
* 噪音刻画了问题本身的难度

方差和偏差具有矛盾性，这就是常说的偏差-方差窘境（bias-variance dilamma），随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大。换句话说：在欠拟合时，偏差主导泛化误差，而训练到一定程度后，偏差越来越小，方差主导了泛化误差。因此训练也不要贪杯，适度辄止。


<img src="{{ site.img_path }}/Machine Learning/feature_engineering12.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>（最上面的线是泛化误差）

### 模型复杂程度和偏差/方差权衡
我们在偏差与方差间都会做出权衡。如果我们的模型过于简单，只有很少的几个参数，那么它可能存在着较大的偏差（但是方差较小）；如果过于复杂而含有非常多的参数，那么模型可能会存在较大的方差（但是偏差较小）

### 损失函数
* 在讲正则化之前，先讲下损失函数，**损失函数:** 用一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度。损失函数是   f (X)和Y的非负实值函数，记作L(Y, f (X)) .
在李航博士《统计学习方法》中有很详细的介绍，我就偷偷懒，就直接贴下那篇图片好啦，然后简单说明下（哈哈，逃。。）

<img src="{{ site.img_path }}/Machine Learning/feature_engineering13.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

损失函数的期望是
<img src="{{ site.img_path }}/Machine Learning/feature_engineering14.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

期望风险Rexp(f)是模型关于联合分布的期望损失，经验风险Remp(f)是模型关于训练样本集的平均损失（也就是概率中是实验数据越大，越能代表一般规律）。根据大数定律，当样本容量N趋于无穷时，经验风险趋于期望风险。所以一个很自然的想法是用经验风险估计期望风险。但是，由于现实中训练样本数目有限，甚至很小，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正.

<img src="{{ site.img_path }}/Machine Learning/feature_engineering15.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>
>当样本容量足够大时，经验风险最小化能保证有很好的学习效果。例：极大似然估计(maximum likelihood estimation)。但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产“过拟合(over-fitting)"现象.这就引出下面的正则化。

### 正则化
>书上说正则化是模型选择的一种方式，但其实我更赞同它的直接目的是防止过拟合！不过我们做的一切都是为了建立选出一个更好的模型是吧！

1. 正则化的目的：防止过拟合！
2. 正则化的本质：约束（限制）要优化的参数。
3. 正则化实现方式：在Cost function误差函数中添加惩罚项
4. 正则化的缺点：正则化防止过拟合，但引入正则化可能会引起“too much regularization”

**问：**对于正则化，优点是使模型“简单”--》这"简单"怎么理解？
**答：**引用李航老师书中的那段话：正则化符合奥卡姆剃刀 (Occam's razor)原理。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应 该选择的模型。从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有较大的先验概率，简单的模型有较小的先验概率。


还是通过例子来描述和理解：下面选自斯坦福大学机器学习公开课
下面是对房子的面积特征和价格的拟合

1. 合适的拟合：

<img src="{{ site.img_path }}/Machine Learning/feature_engineering16.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

2. 过拟合：

<img src="{{ site.img_path }}/Machine Learning/feature_engineering17.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

直观来看，如果我们想解决这个例子中的过拟合问题，最好能将x3,x4的影响消除，也就是让θ3≈0,θ4≈0.
假设我们对θ3,θ4进行惩罚，并且令其很小，一个简单的办法就是给原有的Cost function()加上两个略大惩罚项，例如：


<img src="{{ site.img_path }}/Machine Learning/feature_engineering18.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

这样在最小化Cost function的时候，θ3≈0,θ4≈0.

<img src="{{ site.img_path }}/Machine Learning/feature_engineering19.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

为了使正则化运作良好，我们应当注意一些方面，应该去选择一个不错的正则化参数 λ 。当我们以后讲到多重选择时我们将讨论一种方法来自动选择正则化参数 λ，为了使用正则化，那么我们就可以让他们避免过度拟合了。


### 误差验证-评估方法

最后说下误差验证评估方法，主要包括以下3种

*  **保留交叉验证**，也叫作**留出法**
    1. 从全部的训练数据 S中随机选择 中随机选择 s的样例作为训练集 train，剩余的 作为测试集 作为测试集 test。
    2. 通过对测试集训练 ，得到假设函数或者模型 。
    3. 在测试集对每一个样本根据假设函数或者模型，得到训练集的类标，求出分类正确率。
    4. 选择具有最大分类率的模型或者假设

* **k折交叉验证 k-fold cross validation**
	1. 将全部训练集 S分成 k个不相交的子集，假设 S中的训练样例个数为 m，那么每一个子 集有 m/k 个训练样例，相应的子集称作 {s1,s2,…,sk}。
    2. 每次从分好的子集中里面，拿出一个作为测试集，其它k-1个作为训练集
    3. 根据训练训练出模型或者假设函数。
    4.  把这个模型放到测试集上，得到分类率。
    5. 计算k次求得的分类率的平均值，作为该模型或者假设函数的真实分类率。
    这个方法充分利用了所有样本。但计算比较繁琐，需要训练k次，测试k次。

>当然，交叉验证中有一个比较特殊的情况----**留一法**
>留一法就是每次只留下一个样本做测试集（也就是当上面的k==m的情况），其它样本做训练集，如果有k个样本，则需要训练k次，测试k次。留一发计算最繁琐，但样本利用率最高。适合于小样本的情况。

我们希望评估的是用整个数据集S训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比S小，这必然会引入一些因训 练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。 这就引出下面的自助法

* **自助法**
1. 自助法的基本思想是：给定包含m个样本的数据集S，每次随机从S 中挑选一个样本，将其拷贝放入S’，然后再将该样本放回初始数据集S 中，使得该样本在下次采样时仍有可能被采到。
2. 重复执行m 次，就可以得到了包含m个样本的数据集S’。可以得知在m次采样中，样本始终不被采到的概率取极限为：

<img src="{{ site.img_path }}/Machine Learning/feature_engineering20.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

这样，通过自助采样，初始样本集S中大约有36.8%的样本没有出现在S’中，于是可以将S’作为训练集，S-S’作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入n了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用，我也是最常用交叉验证的。

这次就讲到这里吧，之后我会继续写好下面的博文，希望在写博文的同时，可以加深理解，一起加油！！！

参考资料：

《统计学习方法》-李航<br>
《机器学习》-周志华<br>
《机器学习实战》-Peter Harrington<br>
斯坦福大学公开课-机器学习<br>
网上的各位大牛的博文<br>

  <!-- 多说评论框 start -->
  <div class="ds-thread" data-thread-key="201701181" data-title="feature engineering" data-url=""></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"yzhhome"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共JS代码 end -->

