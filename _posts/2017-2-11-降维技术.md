# 降维技术

* 在我们解决机器学习的问题时,常常要从维度上做文章。有时候我们需要增维, 比如 kernel methods 就可以在高纬度上重构样本从而解决样本在低维上不的线性不可分问题(高斯 kernel 甚至可以把样本在无穷维上展开)。有时候我们又需要降维,因为我们是三维生物,我们最多只能理解三维世界, 所以想要看看手头的数据, 就必须把它们降到三维以内; 另外,往往很多特征没什么用处, 我们可以可以通过降维或者特征工程的手段来把它们剔除,降低计算复杂度。
* 在这篇文章中，我主要讲解PCA，Factor Analysis，和Independent Component Analysis。

## PCA

#### PCA预处理
* PCA算法，不过通常在PCA算法运行前，我们需要对数据进行预处理——对数据的期望及方差进行标准化

1. 整体的数据的期望设为0
2. 各个分量用统一的标准定义，（如一个人的身高，体重...）

>注意，这PCA将白了就是线代中特征分解的应用，所以特征分解要懂，了解可以看下这篇博文[特征分解]()

### PCA算法
* PCA 希望可以找到那 些最重要的维度(假设 k 个重要维度,`k<m`),然后把 X 映射到这些重要维度上,用这 k 个映射系数作为新的 feature 于是样本们就被降低到了 k 维上。
* 假设我们有 N 个样本,每个为 m 维,即 ,怎样定义重要维度呢?假设 u(一个 m 维向量)是一个重要维度,令 u 把 X 射成 这样的 N 个点,u 之所以重要,因为新生成的 N 个点的方差是最大的,于是样本间的差异性就被保存了下来, 我们还是可以在新的样本空间里做聚类,分类等操作。所以 PCA 背后的优化问题其实是,
![](https://pic4.zhimg.com/v2-bdea8aa03480ba7a0dbd983fe0a19983_b.png)



### 所以 PCA 的一般步骤是:
1. 将数据正规化为零期望以及单位化方差；
2. 对样本数据的协方差矩阵(covariance matrix)做计算，得到特征值
3. 特征值 (eigen values) 排序
4. top­k 特征值对应的特征向量 (eigen vectors) 找出来
5. 把 X 映射到这 k 个特征向量上

### SVD（奇异值分解）
* 他是另一种实现PCA的另一种说法
* 可以将数据映射到低维空间，常用于从有噪声数据中抽取相关特征。
* 详细讲解可以看另一个博文：[奇异值分解]()
* 特征分解和奇异值分解：不是所有的矩阵都能对角化（对称矩阵总是可以），而所有矩阵总是可以做奇异值分解的。所以因为这个，导致的是在PCA中，他们两者的是一样的（在这里就是同一个东西），因为协方差矩阵是对称的，然后再其他方面，SVD有更加广泛的应用，比如：在推荐算法方面，可以使得数据集更易使用，简单计算，这个详见《机器学习实战》中SVD分解那一章

### 举例：

注意一开始的数据是三维的：
![](https://pic1.zhimg.com/v2-2c913935c3acc821664ee54b26b857bc_b.png)

经过PCA之后的图：
![](https://pic3.zhimg.com/v2-8da1c89de80399a83370d4dc292fe9fa_b.png)



## Factor Analysis（因子分析）

首先看下它的定义：

因子分析其实就是认为高维样本点实际上是由低维样本点经过高斯分布、线性变换、误差扰动生成的，因此高维数据可以使用低维来表示。是将多个实测变量转换为少数几个不相关的综合指标的多元统计方法。它通过研究众多变量之间的内部依赖关系，探求观测数据中的基本结构，并用少数几个假想变量来表示其基本的数据结构。假想变量是不可观测的潜在变量，称为因子。

Factor Analysis: FA 的思想与 PCA 其实很相似,假设高维度上的观测结果 X 其实是由低维 度上的 factors 来支配的。打个比方,笔者身边有一大群妹子,每个妹子都有很多的参数,例 如,身高,体重,肺活量,皮肤,眼睛大小,脸蛋形状,发型,性格等 8 个参数... 笔者经过大 量的调查研究把每个妹子在每个 feature 上都打了从 1­到10 的分数(10 分最高),然后就在纠 结,到底要对哪个下手呢?于是就想把妹子们做个 ranking,但是只能 rank 一维的数据呀,于 是就在想能不能把妹子的 8 个 feature 抽象成一个终极打分 ­­ 美貌。于是做了如下的假设:
![](https://pic3.zhimg.com/v2-fc4f334fe9df83dc27ff9dad99fd7f92_b.png)

假设每个妹子都有一个终极打分 z(一维),这个分数将会通过一个固定的映射到八个维度 上,然后加上 bias 修正,再加上一些误差(误差保证尽管俩妹子得分一样,也可以春兰秋菊 各有千秋),于是就得到了八维打分 X。这个过程的原理可以让下面这俩图来解释一下: 首先强行把一维的数据搬到二维平面的一条直线上,再加上噪声,bias,于是就 得到了红圈里的一个二维的数据,把二维想象成八维就重构了妹子们的参数。

![](https://pic4.zhimg.com/v2-8b717c199cb9cb0614f511f6c1a600a7_b.png)

有了这个模型,我们就可以就用 EM(expectation­maxminization) 来估算 , 估算过程比较复杂,一句话讲就是通过调整这些参数,令 P(X) 出现的概率最大。 模型确定下来,就可以算出妹子们的最终得分 z, 排个序, 就可以从容地选择了! 继续看下蛋卷图

![](https://pic2.zhimg.com/v2-a19fd559890c5df2842539ff2db05b65_b.png)






* n≫m（特征远大于样本的数）时。在这种情形下，因为样本太少，即使只用一个高斯分布也难以对数据进行建模，更别说用多个高斯分布将数据分开了。特别是因为样本短缺，仅m个样本在Rn空间中只能张成一个维度很低的子空间，如果此时使用高斯分布建模，并使用最大似然估计对期望和协方差做出估计时



## 更严格的规范-对角阵，且值相同
在这种情形下的高斯分布概率密度的等高线图将是正圆形（在二维情况下，而非普通ΣΣ下的椭圆；在高维情况下则是球面或超球面

### 引出因子分析
将Σ限制为对角矩阵也就意味着样本的任意两个分量xi,xj之间是无关且独立的。通常，在不加约束条件时，通过数据拟合得到的Σ将捕获到一些关于数据的有趣的潜在联系，而我们使用上面任意一种约束后，就相当于强行抹掉了样本各分量之间的联系。在后面对因子分析模型的介绍中，我们将在对角矩阵约束之外添加更多参数，用以捕获数据间的联系，但同样不会拟合一个完整的协方差矩阵。
>由此得知因子分析的目的：添加约束抹掉联系，分析其丢失的联系，得到样本之间的联系



### 因子分析模型
[因子分析 ](http://blog.csdn.net/littleqqqqq/article/details/50899717)

[因子分析和主成分分析的差别](http://www.cnblogs.com/appboling/p/3797151.html)


###  因子分析和主成分分析的联系和区别



## ICA
尽管ICA对于服从高斯分布的数据存在这样的缺陷，但只要数据不服从高斯分布，在数据充足的情况下，我们还是可以分离出n个独立的源的

### 我们的目标
1. 使得原始点投影到上的点具有较小的方差。也就是靠的很近,这样会使得（比如离化为的一维直线越远）
2. 也就是说：主成份分析目的就是使得数据点在特定的向量上的投影方差最大。而我们就是要求出这个向量。

