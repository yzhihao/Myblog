---
layout: post
title:  convolution neural network
desc: 我的博客
keywords: 'blog,Machine Learning,AI'
date: 2017-3-18T00:00:00.000Z
categories:
  - Machine Learning
tags:
  - Machine Learning
  - AI
icon: fa-book
---


## 目录
**欢迎在文章下方评论，建议用电脑看**

* 目录
{:toc}

## convolution neural network

卷积神经网络和上一章讲的常规神经网络非常相似：它们都是由神经元组成，神经元中有具有学习能力的权重和偏差。每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。

## 四类解决方案（from PRML）
• Data Augmentation：用神经网络生成多种变化
• Tangent Propagation：用正则项的方式，用一个函数
• Invariant feature：用一些特征用来做不变性
• Neural Network structure with invariant properties (e.g. CNN)：用一些特征结构，比如cnn

### BackGround

- 人眼在识别图像时，往往从局部到全局
- 局部与局部之间联系往往不太紧密
- 我们不需要神经网络中的每个结点都掌握全局的知识，因此可以从这里减少需要学习的参数数量

## 简介

* 卷积神经网络主要由三种类型的层构成：卷积层，汇聚（Pooling）层和全连接层（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。
* 有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）。
* 有的层有额外的超参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）。

下面是一个cnn神经网络结构的例子：
![](https://pic3.zhimg.com/d9259be829b1cdb3d98a399ebc56defa_b.jpg)

下面以层到层的顺序来讲解cnn：

## 卷积层

注意，一下讨论的前提是你熟悉了传统ann的网络结构和一般知识，不明白的可以先看下我前面的博文，[传送门]()

## 卷积计算

* 其实，深度神经网络就是隐含层的数量较多，导致参数增多多，而我们可以理解卷积层存在的重要意义就是减少参数的数量。

* 首先讨论的是，在没有大脑和生物意义上的神经元之类的比喻下，卷积层到底在计算什么。卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是**深度和输入数据一致**

* 在深度一致的时候，n*n的fiter对m*m的图片做卷积运算的话，就会产生（m-n）/1+1（注意那个除以1是步长，在这里步长是1，当然也可以设置成其他的值，除不尽的情况是在外围加上几层，使得可以整除）的新的生成层，所以最重要的就是能整除

举个例子：
 假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。但这样有什么用呢？请看下面：

### 局部链接
左图为全连接，右图为局部连接：
![](https://raw.githubusercontent.com/stdcoutzyx/Paper_Read/master/blogs/imgs/5.jpg)
在上右图中，假如每个神经元只和10×10个像素值相连，那么权值数据为1000000×100个参数，减少为原来的万分之一。而那10×10个像素值对应的10×10个参数，其实就相当于卷积操作。所以，这样做的目的就是大大的减少了参数的数量。


### 参数共享

参数共享的原因:
如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也会同样是有用的，这是因为图像结构具有平移不变性。所以在卷积层的输出数据体的55x55个不同位置中，就没有必要重新学习去探测一个水平边界了。也就是说如果说一个卷积核在图片的一小块儿区域可以得到很好的特征，那么在其他的地方，也可以得到很好的特征。

真实案例：Krizhevsky构架赢得了2012年的ImageNet挑战，其输入图像的尺寸是[227x227x3]。在第一个卷积层，神经元使用的感受野尺寸，步长，不使用零填充。因为(227-11)/4+1=55，卷积层的深度，则卷积层的输出数据体尺寸为[55x55x96]。55x55x96个神经元中，每个都和输入数据体中一个尺寸为[11x11x3]的区域全连接。在深度列上的96个神经元都是与输入数据体中同一个[11x11x3]区域连接，但是权重不同。有一个有趣的细节，在原论文中，说的输入图像尺寸是224x224，这是肯定错误的，因为(224-11)/4+1的结果不是整数。这件事在卷积神经网络的历史上让很多人迷惑，而这个错误到底是怎么发生的没人知道。我的猜测是Alex忘记在论文中指出自己使用了尺寸为3的额外的零填充。

* 在卷积层中使用参数共享是用来控制参数的数量。就用上面的例子，在第一个卷积层就有55x55x96=290,400个神经元，每个有11x11x3=364个参数和1个偏差。将这些合起来就是290400x364=105,705,600个参数。单单第一层就有这么多参数，显然这个数目是非常大的。
* 在一个深度切片中，每个滤波器都被55x55个神经元共享。注意参数共享的假设是有道理的：如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也会同样是有用的，这是因为图像结构具有平移不变性。共有96x11x11x3=34,848个不同的权重，或34,944个参数（+96个偏差）

>相对全连接传统神经网络，ｃｎｎ的参数较少的原理就是参数共享和局部连接

>**注意:**如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的卷积（这就是“卷积层”名字由来）。这也是为什么总是将这些权重集合称为滤波器（filter）（或卷积核（kernel）），因为它们和输入进行了卷积。可以看做就是点乘！

### 多卷积核

* 可以认为多个卷积和就是在提取不同的方面的特征！
* 上面所述只有100个参数时，表明只有1个10*10的卷积核，显然，特征提取是不充分的，我们可以添加多个卷积核，比如32个卷积核，可以学习32种特征。在有多个卷积核时，如下图所示：
![](https://raw.githubusercontent.com/stdcoutzyx/Paper_Read/master/blogs/imgs/7.jpg)
关于卷积层的讲解，当然其他也讲的很好，大家可以看下cs231n课程中的笔记，特别是那个gif图片，讲两个卷积核详细的卷积运算过程，[英文版](https://cs231n.github.io/convolutional-networks/#conv);[中文翻译](https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit)

## 一些卷积层的trick&tip

如下：

### 1. 我们倾向于选择多层小size的卷积层，而不是一个大size的卷积层。
现在，我们以3个3x3的卷积层和1个7x7的卷积层为例，加以对比说明。从下图可以看出，这两种方法最终得到的activation map大小是一致的，但3个3x3的卷积层明显更好：

    1)、3层的非线性组合要比1层线性组合提取出的特征具备更高的表达能力；
    2)、3层小size的卷积层的参数数量要少，3x3x3<7x7；
    3)、同样的，为了便于反向传播时的梯度计算，我们需要保留很多中间梯度，3层小size的卷积层需要保留的中间梯度更少。
![](http://upload-images.jianshu.io/upload_images/2301760-f4dac7749cd64ada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 1×1的卷积大概有两个方面的作用
1. 实现跨通道的交互和信息整合
2. 进行卷积核通道数的降维和升维

>这个详见[ImageNet Evolution]()


## 汇聚层
1. 在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，**使得计算资源耗费变少，也能有效控制过拟合，提取重要特征，符合不变性**。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。
2. 一般是2*2或者3*3，计算公式和卷积层的差不多

![](https://pic4.zhimg.com/641c8846abcb02d35938660cf96cef1b_b.jpg)

通常，池化层的采样窗口大小为2x2。

有些人认为池化层并不是必要的，此外，有人发现去除池化层对于生成式模型（generative models）很重要，例如variational autoencoders(VAEs)，generative adversarial networks(GANs)。可能在以后的模型结构中，池化层会逐渐减少或者消失。


## 归一化层
在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的。

## OutPut
将一个deep and narrow的feature层作为输入，传给一个Regular神经网络
![](https://github.com/yzhihao/GDLnotes/raw/master/res/conv_output.png)

## Optimization
### Pooling
将不同Stride的卷积用某种方式合并起来，节省卷积层的空间复杂度。

- Max Pooling
在一个卷积层的输出层上取一个切片，取其中最大值代表这个切片
- 优点
  - 不增加需要调整的参数
  - 通常比其他方法准确
- 缺点：更多Hyper Parameter，包括要取最值的切片大小，以及去切片的步长

## 理解dropout

开篇明义，dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。

dropout是CNN中防止过拟合提高效果的一个大杀器，但对于其为何有效，却众说纷纭。在下读到两篇代表性的论文，代表两种不同的观点，特此分享给大家。

> LENET-5, ALEXNET

- Average Pooling
在卷积层输出中，取切片，取平均值代表这个切片

### 1x1 Convolutions
在一个卷积层的输出层上，加一个1x1的卷积层，这样就形成了一个小型的神经网络。
- cheap for deeper model
- 结合Average Pooling食用效果更加

### Inception
对同一个卷积层输出，执行各种二次计算，将各种结果堆叠到新输出的depth方向上
![](https://github.com/yzhihao/GDLnotes/raw/master/res/inception.png)

## ＣＮＮ 的结构

最简单的CNNs结构的diagram（input+1conv+1pool+2fc）:
![](http://upload-images.jianshu.io/upload_images/2301760-029d98a6f80dda80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里我们列举几种常见类型的卷积神经网络结构：

    · INPUT --> FC/OUT      这其实就是个线性分类器
    · INPUT --> CONV --> RELU --> FC/OUT
    · INPUT --> [CONV --> RELU --> POOL]*2 --> FC --> RELU --> FC/OUT
    · INPUT --> [CONV --> RELU --> CONV --> RELU --> POOL]*3 --> [FC --> RELU]*2 --> FC/OUT

## 深度残差网络
残差网络（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的跳跃链接，大量使用了批量归一化（batch normalization）。这个结构同样在最后没有使用全连接层。读者可以查看何恺明的的演讲（视频，PPT），以及一些使用Torch重现网络的实验。ResNet当前最好的卷积神经网络模型（2016年五月）。何开明等最近的工作是对原始结构做一些优化，可以看论文Identity Mappings in Deep Residual Networks，2016年3月发表。

>可以看作很多简单网络的组成的网络

## 参考链接
- 张雨石 [Conv神经网络](http://blog.csdn.net/stdcoutzyx/article/details/41596663)
- Bill Xia [卷积神经网络（CNN）](https://ibillxia.github.io/blog/2013/04/06/Convolutional-Neural-Networks/)



<!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="2017031802" data-title="convolution neural network" data-url=""></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"yzhhome"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共JS代码 end -->
