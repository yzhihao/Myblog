# N-Gram模型

## 作用

基于N-Gram模型定义的字符串距离
利用N-Gram模型评估语句是否合理
使用N-Gram模型时的数据平滑算法


来自[自然语言处理中的N-Gram模型详解](http://blog.csdn.net/baimafujinji/article/details/51281816)

## 利用N-Gram模型评估语句是否合理

从现在开始，我们所讨论的N-Gram模型跟前面讲过N-Gram模型从外在来看已经大不相同，但是请注意它们内在的联系（或者说本质上它们仍然是统一的概念）

为了引入N-Gram的这个应用，我们从几个例子开始。
首先，从统计的角度来看，自然语言中的一个句子 s 可以由任何词串构成，不过概率 P<sub>(s)</sub> 有大有小。例如：

s<sub>1</sub> = 我刚吃过晚饭
s<sub>2</sub> = 刚我过晚饭吃

显然，对于中文而言 s<sub>1</sub> 是一个通顺而有意义的句子，而s2 则不是，所以对于中文来说，P(s<sub>1</sub>)>P(s<sub>2</sub>) 。但不同语言来说，这两个概率值的大小可能会反转。

其次，另外一个例子是，如果我们给出了某个句子的一个节选，我们其实可以能够猜测后续的词应该是什么，例如

* the large green __ . Possible answer may be “mountain” or “tree” ?
* Kate swallowed the large green __ . Possible answer may be “pill” or “broccoli” ?

显然，如果我们知道这个句子片段更多前面的内容的情况下，我们会得到一个更加准确的答案。这就告诉我们，**前面的（历史）信息越多，对后面未知信息的约束就越强。**

如果我们有一个由 m 个词组成的序列（或者说一个句子），我们希望算得概率 P(w<sub>1</sub>,w<sub>2</sub>,⋯,w<sub>m</sub>) ，根据链式规则，可得 

<img src="{{ site.img_path }}/Machine Learning/N-Gram.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

这个概率显然并不好算，不妨利用马尔科夫链的假设，即当前这个词仅仅跟前面几个有限的词相关，因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上诉算式的长度。即

<img src="{{ site.img_path }}/Machine Learning/N-Gram1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

接下来的思路就比较明确了，可以利用最大似然法来求出一组参数，使得训练样本的概率取得最大值。

<img src="{{ site.img_path }}/Machine Learning/N-Gram2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

再举一个例子，假设现在有一个语料库，我们统计了下面一些词出现的数量

<img src="{{ site.img_path }}/Machine Learning/N-Gram3.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>