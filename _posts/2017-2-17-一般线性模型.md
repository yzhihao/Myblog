---
layout: post
title: 一般线性模型
desc: 我的博客
keywords: 'blog,Machine Learning,AI'
date: 2017-2-17T00:00:00.000Z
categories:
  - Machine Learning
tags:
  - Machine Learning
  - AI
icon: fa-book
---


## 目录
**欢迎在文章下方评论，建议用电脑看**

* 目录
{:toc}

# 一般线性模型

好了，下来我们来讲讲一般线性模型！首先来看看对于分类问题，我们也可以无视y是取离散值的特征而继续使用线性回归，使用老办法通过x预测y的取值。然而，强行使用线性回归的方法处理分类问题通常会得到很糟糕的预测结果。


## 逻辑回归

### 逻辑回归和线性回归
* 逻辑回归就是在线性回归上加一个函数。就是一个复合函数
* 逻辑函数不不能直接向线性回归那样定义代价函数，因为那样不是凸优化的函数，有局部最优解

Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为：

<img src="{{ site.img_path }}/Machine Learning/logic_R1.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

Sigmoid 函数在有个很漂亮的“S”形，如下图所示（引自维基百科）：

<img src="{{ site.img_path }}/Machine Learning/logic_R2.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


* 预测函数：
	![](http://img.blog.csdn.net/20140716153431593)

在线性回归中我们知道了，从一些列假设下的最大似然估计可以推导出最小二乘回归的合理性，所以，我们也可以按照这个思路，给我们的分类模型赋予一些列概率假设，然后通过最大似然估计拟合参数。假设：


<img src="{{ site.img_path }}/Machine Learning/logic_R3.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

我将这两个式子结合一下：


<img src="{{ site.img_path }}/Machine Learning/logic_R4.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


取似然函数为：

<img src="{{ site.img_path }}/Machine Learning/logic_R5.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


对数似然函数为：

<img src="{{ site.img_path }}/Machine Learning/logic_R6.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


有了似然函数，我们用梯度上升的方法来求解极大似然值：



<img src="{{ site.img_path }}/Machine Learning/logic_R7.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>



<img src="{{ site.img_path }}/Machine Learning/logic_R8.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>




* 决策边界的确定，这个主要是和x的多少此方有关，详细看[Decision Boundary](https://www.coursera.org/learn/machine-learning/supplement/N8qsm/decision-boundary)

θ更新过程可以写成：


<img src="{{ site.img_path }}/Machine Learning/logic_R9.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>



## Softmax回归
我们知道Logistic回归只能进行二分类，因为它的随机变量的取值只能是0或者1，那么如果我们面对多分类问题怎么
办？比如要将一封新收到的邮件分为垃圾邮件，个人邮件，还是工作邮件；根据病人的病情预测病人属于哪种病；对于
诸如MNIST手写数字分类（[MNIST是一个手写数字识别库](http://yann.lecun.com/exdb/mnist/)）。
诸如此类问题都涉及到多分类，而softmax回归能解决这类问题。

Softmax回归就是逻辑回归一个推广，其预测函数是：

<img src="{{ site.img_path }}/Machine Learning/logic_R10.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

接下来得到对数似然函数函数为:

<img src="{{ site.img_path }}/Machine Learning/logic_R11.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

在softmax回归中直接用上述对数似然函数是不能更新参数的，因为它存在冗余的参数，通常用牛顿方法中的Hessian矩阵也不可逆，是一个非凸函数，那么可以通过添加一个权重衰减项来修改代价函数，使得代价函数是凸函数，并且得到的Hessian矩阵可逆。


Softmax回归就讲这么多，如果还想了解更多的话，详见下面的两篇较好的博文：

>[Softmax回归-博文1](http://blog.csdn.net/acdreamers/article/details/44663305)
>[Softmax回归-博文2](http://blog.csdn.net/acdreamers/article/details/44663305)



### The exponential family 指数分布族

因为广义线性模型是围绕指数分布族的，因此需要先介绍，用NG大神的话说就是，“虽然不是全部，但是我们见过的大多数分布都属于指数分布族，比如：Bernoulli伯努利分布、Gaussian高斯分布、multinomial多项分布、Poisson泊松分布、gamma分布、指数分布、Dirichlet分布……”服从指数分布族的条件是概率分布可以写成如下形式：

<img src="{{ site.img_path }}/Machine Learning/logic_R12.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


η 被称作natural parameter，它是指数分布族唯一的参数
T(y) 被称作sufficient statistic，很多情况下T(y)=y a(η) 被称作 log partition function
T函数、a函数、b函数共同确定一种分布
接下来看一下为什么说正态分布（高斯分布）属于指数分布族：
**正态分布**（正态分布有两个参数μ均值与σ标准差，在做线性回归的时候，我们关心的是均值而标准差不影响模型的学习与参数θ的选择，因此这里将σ设为1便于计算）

<img src="{{ site.img_path }}/Machine Learning/logic_R13.jpeg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


**构成广义线性模型的三个假设**

1. p(y | x; θ) ∼ ExponentialFamily(η). 输出变量基于输入变量的条件概率分布服从指数分布族
2. our goal is to predict the expected value of T(y) given x. 对于给定的输入变量x，学习的目标是预测T(y)的期望值，T(y)经常就是y
3. The natural parameter η and the inputs x are related linearly: η = θT x. η和输入变量x的关联是线性的：η = θ^T x


目的：由问题，用指数族分布一般式推出x->y的关系式。注意是什么和什么的关系,从而得到模型

* 多项分布（multinomial），我们将在后面介绍，如同伯努利分布对有两个结果的事件建模一样，多项分布对有K个结果的事件建模；
* 泊松分布（Poisson），用于计数数据建模，比如样本中放射性元素衰变的书目、某网站访客的数量、商店顾客的数量；
* 伽马分布和指数分布（the gamma and the exponential），用于非负的连续随机变量建模，如处理时间间隔，或是处理在等公交时发生的“下一辆车什么时候到”的问题；
* 贝塔分布和狄利克雷分布（the beta and the Dirichlet），通常用于对分数进行建模，它是对概率分布进行建模的概率分布；
* Wishart分布，这是协方差矩阵的分布。

一般的，对于想要预测的关于x的随机变量y的分类或回归问题，我们为了推导出关于问题的一般线性模型。


求特定线性模型的步骤：写出y∣x;θ符合的分布公式->还原出指数族分布一般式->找到E[y∣x;θ]和自然参数的关系->由自然参数和输入特征值x是线性关系η=θTx->得到模型公式

### 一般线性模型的举例

####  线性回归：

这三个假设其实指明了如何从输入变量映射到输出变量与概率模型，举例来说：线性回归的条件概率分布为正态分布属于指数分布族（参考笔记一中线性回归的似然函数部分）；我们的目标是预测T(y)的期望，由上面的计算我们知道T(y)=y，而y的期望值也就是正态分布的参数μ；由上面的计算我们知道μ=η，而η=θT x。因此，线性回归是广义线性回归的一个特例，它的模型是：

<img src="{{ site.img_path }}/Machine Learning/logic_R14.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>



#### Logistic Regression 逻辑回归


逻辑回归解决的是分类问题，并且是只分为两类。比如：用户是否会点击广告链接？用户是否会回访？抛一枚硬币正面是否会朝上？于是，从概率的观点出发，我们应该马上想到用伯努利分布来估计事件发生的概率。从广义线性模型的角度来构造逻辑回归模型：
1.1 伯努力分布属于指数分布族（参数Φ指的是y=1的概率，即事件发生的概率）

<img src="{{ site.img_path }}/Machine Learning/logic_R15.jpeg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


1.2 学习的目标是预测T(y)的期望值，而伯努利分布中T(y)=y，另外我们知道伯努利分布的期望就是参数Φ，即E(y)=Φ。
1.3 由η = log(Φ/(1 – Φ) 可以推出 Φ = 1/(1+e-η) （这就是所谓的logistic函数，也是逻辑回归名字的来由），再将η=θT x带入公式，最终我们得到逻辑回归的模型：

<img src="{{ site.img_path }}/Machine Learning/logic_R16.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

因为伯努利分布的参数Φ既是分布的期望，又代表事件发生的概率，因此逻辑回归模型的意义就是：在给定的输入变量组合的条件下，输出变量（二元变量）中一个事件发生的概率。比如：预测在用户是第一次来访（输入变量1），广告链接用的是热门文案（输入变量2）的条件下，广告链接被点击（输出变量）的概率为多少。 看到这里，相信大家应该能够明白：为什么逻辑函数要长成这样，为什么逻辑回归能起作用了吧。

## 牛顿法
从高数中知道极值点就是导数为0的地方，所以最大化对数似然函数的另一个求法是求对数似然函数导数为0的点。而牛顿方法就是求得对数似然函数导数为0的点的方法：


<img src="{{ site.img_path }}/Machine Learning/logic_R17.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

当参数θ只有一个时，牛顿方法的迭代规则：

<img src="{{ site.img_path }}/Machine Learning/logic_R18.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

当参数θ不止一个时，牛顿方法的迭代规则：

<img src="{{ site.img_path }}/Machine Learning/logic_R19.jpeg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


H是一个n阶方阵（如果算上截距项应该是n+1n+1阶方阵），称为海森矩阵（Hessian matrix），为了便于记忆可以当做是一阶导数（向量）除以二阶导数（乘以方阵的逆）

相较于批量梯度下降，牛顿方法通常来说有更快的收敛速度，只需要少得多的迭代次数就能得到很接近最小值的结果。但是当模型的参数很多时（参数个数为n）Hessian矩阵的计算成本将会很大，导致收敛速度变慢，但是当参数个数不多时，牛顿方法通常是比梯度下降快得多的。

**伪牛顿法**

上述的牛顿法需要计算Hessian矩阵的逆矩阵，运算复杂度太高。在动辄百亿、千亿量级特征的大数据时代，模型训练耗时太久。因此，很多牛顿算法的变形出现了，这类变形统称拟牛顿算法。拟牛顿算法的核心思想用一个近似矩阵B替代逆Hessian矩阵H−1。不同算法的矩阵B的计算有差异，但大多算法都是采用迭代更新的思想在tranning的没一轮更新矩阵B。

>由于这些伪牛顿法比较复杂，而且在机器学习中也较少用，所以在这里先不细讲了，有兴趣的小伙伴可以看下[这篇博文](http://www.cnblogs.com/richqian/p/4535550.html)


## 最小二乘和高斯-牛顿法

最后，来看下最小二乘和那些优化算法的guanxi

最小二乘法的目标：求误差的最小平方和，对应有两种：线性和非线性。线性最小二乘的解是closed-form即，而非线性最小二乘没有closed-form，通常用迭代法求解。

迭代法，即在每一步update未知量逐渐逼近解，可以用于各种各样的问题（包括最小二乘），比如求的不是误差的最小平方和而是最小立方和。

梯度下降是迭代法的一种，可以用于求解最小二乘问题（线性和非线性都可以）。高斯-牛顿法是另一种经常用于求解非线性最小二乘的迭代法（一定程度上可视为标准非线性最小二乘求解方法）。

还有一种叫做Levenberg-Marquardt的迭代法用于求解非线性最小二乘问题，就结合了梯度下降和高斯-牛顿法。

所以如果把最小二乘看做是优化问题的话，那么梯度下降是求解方法的一种，是求解线性最小二乘的一种，高斯-牛顿法和Levenberg-Marquardt则能用于求解非线性最小二乘。

>看出牛顿法和梯度下降都是求最小二乘的一种方式，最终目的就是求参数