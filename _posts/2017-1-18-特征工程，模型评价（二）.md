---
layout: post
title: 特征工程，模型评价（二）
desc: 我的博客
keywords: 'blog,Machine Learning,AI'
date: 2017-2-11T00:00:00.000Z
categories:
  - Machine Learning
tags:
  - Machine Learning
  - AI
icon: fa-book
---


## 目录
**欢迎在文章下方评论，建议用电脑看**

* 目录
{:toc}

## 特征工程
坊间戏言“特征没做好,参数调到老”,机器学习大牛 Andrew Ng 也说过“‘Applied
machine learning’is basically feature engineering”,可见特征工程的重要性,我们要在在这部分投入了大量的时间和精力。下面先讲讲怎么来对特征进行处理

## 编码方式

**one-hot编码：**大多使用一些线性的算法，它的稀疏格式是记忆友好的(便于存储)，注意去掉第一列避免共线性
**哈希编码：**它是对固定长度的数组进行 Onehot 编码，它避免极度稀疏的数据，但可能会产生冲突
**标签编码：**它给每个类一个独一无二的数字化 ID,它对于对于**非线性的基于树模型**的算法很有用，它不增加维度
**Count 编码:Replace categorical variables with their count in the train set**,它对线性或非线性的算法都适用，对对异常值敏感，**Replace unseen variables with `1`**
**LabelCount 编码:Rank categorical variables by count in train set**,对线性或非线性算法都适用,它对异常值不敏感

>在这里讲述的是常见的几种编码方式，其实还有其他的各种不同的编码，具体请看

## Numerical Features（数值化特征）

Can be more readily fed into algorithms，Can constitute floats, counts, numbers and Easier to impute missing data.

## 装箱
把数字化变量放入箱中,并用 bin-ID 编码,用分位数装箱是很实用的,甚至可以用模型找出可选的箱
可以优雅的找到训练集范围外的变量

### 离散特征：

特征离散化有两种划分方式:一种是等值划分(按照值域均分),另一种是等量划分
(按照样本数均分)。我们对 numeric 类型的特征采用了等量划分的离散化方式:先将每
一维特征按照数值大小排序,然后均匀地划分为 10 个区间,即离散化为 1~10。


### 1. 排序特征：

排序特征对异常数据都有较强的鲁棒性,使得模型更加稳定,降低过拟合的风险。



### 计数特征

前面已经对特征进行了离散化,以 uid 为 1 的样本为例,离散化后它的特征是
5,3,1,3,3,3,2,4,3,2,5,3,2,3,2...2,2,2,2,2,2,2,可以进一步统计离散特征中 1~10 出现的次数
n i (i=1,2,...,10),即可得到一个 10 维计数特征。基于这 10 维特征训练了 xgboost 分类器,线
上得分是 0.58 左右,说明这 10 维特征具有不错的判别性。

### 类别特征编码

赛题数据含有 93 维类别特征,很多算法(如逻辑回归,SVM)只能处理数值型特征,
这种情况下需要对类别特征进行编码,我们采用了 One-Hot 编码,得到了 01 特征,解决
了分类器不能处理类别特征的问题。

当不清楚这个编码的时候，你就很有可能会犯错误，例如颜色属性可能会用{1,2,3}表示{红，绿，蓝}。这里存在两个问题：首先，对于一个数学模型，这意味着某种意义上红色和绿色比和蓝色更“相似”（因为|1-3| > |1-2|）。除非你的类别拥有排序的属性（比如铁路线上的站），这样可能会误导你的模型。

### 交叉特征:

交叉特征算是特征工程中非常重要的方法之一了，交叉特征是一种很独特的方式，它将两个或更多的类别属性组合成一个。当组合的特征要比单个特征更好时，这是一项非常有用的技术。数学上来说，是对类别特征的所有可能值进行交叉相乘。

假如拥有一个特征A，A有两个可能值{A1,A2}。拥有一个特征B，存在{B1,B2}等可能值。然后，A&B之间的交叉特征如下：{(A1,B1),(A1,B2),(A2,B1),(A2,B2)}，并且你可以给这些组合特征取任何名字。但是需要明白每个组合特征其实代表着A和B各自信息协同作用。

一个更好地诠释好的交叉特征的实例是类似于(经度,纬度)。一个相同的经度对应了地图上很多的地方，纬度也是一样。但是一旦你将经度和纬度组合到一起，它们就代表了地理上特定的一块区域，区域中每一部分是拥有着类似的特性。

## 缺失值填充--难点







## 	评价分类器性能指标之AUC、ROC

### 引子

假设有下面两个分类器，哪个好？（样本中有A类样本90个，B 类样本10个。）

<img src="{{ site.img_path }}/Machine Learning/roc_auc3.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

分类器C1把所有的测试样本都分成了A类，分类器C2把A类的90个样本分对了70个，B类的10个样本分对了5个。

则C1的分类精度为 90%，C2的分类精度为75%，但直觉上，我们感觉C2更有用些。但是依照正确率来衡量的话，那么肯定C1的效果好一点。那么这和我们认为的是不一致的。也就是说，有些时候，仅仅依靠正确率是不妥当的。

我们还需要一个评价指标，**能客观反映对正样本、负样本综合预测的能力，还要考虑消除样本倾斜的影响（其实就是归一化之类的思想，实际中很重要，比如pv总是远远大于click），这就是auc指标能解决的问题。**

### ROC

为了理解auc，我们需要先来弄懂ROC。
先来看一个普遍的二分类问题的结果，预测值和实际值有4种组合情况，看下面的表格：
![](http://7xkmkg.com1.z0.glb.clouddn.com/svm005.jpg)

>**注意这里就只有两个分类，所以FN就表示的是分错的pos类，同理FP就表示的是分错的neg类**

我们定义一个变量：

<img src="{{ site.img_path }}/Machine Learning/roc_auc1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

看图也就可以知道：

1. TPR表示的就是预测正确且实际分类为正的数量 与 所有正样本的数量的比例。--实际的正样本中，正确预测的比例是多少？
2. FPR表示的是预测错误且实际分类为负的数量 与所有负样本数量的比例。 --实际的负样本当中，错误预测的比例是多少？

下面来看一则对话来理解recell，accuracy和precison：

    What percent of your predictions were correct?
    You answer: the "accuracy" was (TP+TN)/ALL
    What percent of the positive cases did you catch?
    You answer: the "recall" TP/(TP+FN)
    What percent of positive predictions were correct?
    You answer: the "precision" was TP/(TP+FP)

可以代入到上面的两个分类器当中，可以得到下面的表格（分类器C1）：


<img src="{{ site.img_path }}/Machine Learning/roc_auc4.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

TPR = FPR = 1.0

分类器C2：

<img src="{{ site.img_path }}/Machine Learning/roc_auc2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

TPR = 0.78， FPR = 0.5

那么，以TPR为纵坐标，FPR为横坐标画图，可以得到：

![](http://7xkmkg.com1.z0.glb.clouddn.com/svm006.jpg)

上图中蓝色表示C1分类器，绿色表示C2分类器。可以知道，这个时候绿色的点比较靠近左上角，可以看做是分类效果较好。所以评估标准改为离左上角近的是好的分类器（考虑了正负样本的综合分类能力）。

一连串这样的点构成了一条曲线，该曲线就是ROC曲线。而ROC曲线下的面积就是AUC（Area under the curve of ROC）。这就是AUC指标的由来。

### 如何画ROC曲线

对于一个特定的分类器和测试数据集，显然只能得到一个分类结果，即一组FPR和TPR结果，而要得到一个曲线，我们实际上需要一系列FPR和TPR的值才能得到这样的曲线，这又是如何得到的呢？

可以通过分类器的一个重要功能“概率输出”，即表示分类器认为某个样本具有多大的概率属于正样本（或负样本），来动态调整一个样本是否属于正负样本（还记得当时阿里比赛的时候有一个表示被判定为正样本的概率的列么？）

假如我们已经得到了所有样本的概率输出（属于正样本的概率），现在的问题是如何改变这个阈值（概率输出）？我们根据每个测试样本属于正样本的概率值从大到小排序。下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。

![](http://static.oschina.net/uploads/img/201411/03210218_wDRH.png)

接下来，我们从高到低，依次将“Score”值作为阈值，当测试样本属于正样本的概率大于或等于这个阈值时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的阈值，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值，将它们画在ROC曲线的结果如下图：

![](http://static.oschina.net/uploads/img/201411/03210223_awZE.png)

当我们将阈值设置为1和0时，分别可以得到ROC曲线上的(0,0)和(1,1)两个点。将这些(FPR,TPR)对连接起来，就得到了ROC曲线。当阈值取值越多，ROC曲线越平滑。

--在阿里比赛的时候还以为ROC是没用的！！！！真的是有眼无珠啊！！！还是有疑惑的是：如何根据ROC来判定结果的好换呢？看哪个分类器更加接近左上角吧。同时，可以根据ROC来确定划定正样本的概率边界选择在哪里比较合适！！！原来是这样！！！！！！！！！

### 为什么使用ROC

既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比：

![](http://static.oschina.net/uploads/img/201411/03210228_yobH.png)

在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。(a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大。



## 降维技术

* 在我们解决机器学习的问题时,常常要从维度上做文章。有时候我们需要增维, 比如 kernel methods 就可以在高纬度上重构样本从而解决样本在低维上不的线性不可分问题(高斯 kernel 甚至可以把样本在无穷维上展开)。有时候我们又需要降维,因为我们是三维生物,我们最多只能理解三维世界, 所以想要看看手头的数据, 就必须把它们降到三维以内; 另外,往往很多特征没什么用处, 我们可以可以通过降维或者特征工程的手段来把它们剔除,降低计算复杂度。
* 在这篇文章中，我主要讲解PCA，Factor Analysis，和Independent Component Analysis。

### PCA

#### PCA预处理
* PCA算法，不过通常在PCA算法运行前，我们需要对数据进行预处理——对数据的期望及方差进行标准化

1. 整体的数据的期望设为0
2. 各个分量用统一的标准定义，（如一个人的身高，体重...）

>注意，这PCA将白了就是线代中特征分解的应用，所以特征分解要懂，了解可以看下这篇博文[特征分解]()

### PCA算法
* PCA 希望可以找到那 些最重要的维度(假设 k 个重要维度,`k<m`),然后把 X 映射到这些重要维度上,用这 k 个映射系数作为新的 feature 于是样本们就被降低到了 k 维上。
* 假设我们有 N 个样本,每个为 m 维,即 ,怎样定义重要维度呢?假设 u(一个 m 维向量)是一个重要维度,令 u 把 X 射成 这样的 N 个点,u 之所以重要,因为新生成的 N 个点的方差是最大的,于是样本间的差异性就被保存了下来, 我们还是可以在新的样本空间里做聚类,分类等操作。所以 PCA 背后的优化问题其实是,
* 
![](https://pic4.zhimg.com/v2-bdea8aa03480ba7a0dbd983fe0a19983_b.png)



### 所以 PCA 的一般步骤是:
1. 将数据正规化为零期望以及单位化方差；
2. 对样本数据的协方差矩阵(covariance matrix)做计算，得到特征值
3. 特征值 (eigen values) 排序
4. top­k 特征值对应的特征向量 (eigen vectors) 找出来
5. 把 X 映射到这 k 个特征向量上

### SVD（奇异值分解）
* 他是另一种实现PCA的另一种说法
* 可以将数据映射到低维空间，常用于从有噪声数据中抽取相关特征。
* 详细讲解可以看另一个博文：[奇异值分解]()
* 特征分解和奇异值分解：不是所有的矩阵都能对角化（对称矩阵总是可以），而所有矩阵总是可以做奇异值分解的。所以因为这个，导致的是在PCA中，他们两者的是一样的（在这里就是同一个东西），因为协方差矩阵是对称的，然后再其他方面，SVD有更加广泛的应用，比如：在推荐算法方面，可以使得数据集更易使用，简单计算，这个详见《机器学习实战》中SVD分解那一章

### 举例：

注意一开始的数据是三维的：
![](https://pic1.zhimg.com/v2-2c913935c3acc821664ee54b26b857bc_b.png)

经过PCA之后的图：
![](https://pic3.zhimg.com/v2-8da1c89de80399a83370d4dc292fe9fa_b.png)



## Factor Analysis（因子分析）


Factor Analysis: FA 的思想与 PCA 其实很相似,假设高维度上的观测结果 X 其实是由低维 度上的 factors 来支配的。打个比方,笔者身边有一大群妹子,每个妹子都有很多的参数,例 如,身高,体重,肺活量,皮肤,眼睛大小,脸蛋形状,发型,性格等 8 个参数... 笔者经过大 量的调查研究把每个妹子在每个 feature 上都打了从 1­到10 的分数(10 分最高),然后就在纠 结,到底要对哪个下手呢?于是就想把妹子们做个 ranking,但是只能 rank 一维的数据呀,于 是就在想能不能把妹子的 8 个 feature 抽象成一个终极打分 ­­ 美貌。于是做了如下的假设:
![](https://pic3.zhimg.com/v2-fc4f334fe9df83dc27ff9dad99fd7f92_b.png)

假设每个妹子都有一个终极打分 z(一维),这个分数将会通过一个固定的映射到八个维度 上,然后加上 bias 修正,再加上一些误差(误差保证尽管俩妹子得分一样,也可以春兰秋菊 各有千秋),于是就得到了八维打分 X。这个过程的原理可以让下面这俩图来解释一下: 首先强行把一维的数据搬到二维平面的一条直线上,再加上噪声,bias,于是就 得到了红圈里的一个二维的数据,把二维想象成八维就重构了妹子们的参数。

![](https://pic4.zhimg.com/v2-8b717c199cb9cb0614f511f6c1a600a7_b.png)

有了这个模型,我们就可以就用 EM(expectation­maxminization) 来估算 , 估算过程比较复杂,一句话讲就是通过调整这些参数,令 P(X) 出现的概率最大。 模型确定下来,就可以算出妹子们的最终得分 z, 排个序, 就可以从容地选择了! 继续看下蛋卷图

![](https://pic2.zhimg.com/v2-a19fd559890c5df2842539ff2db05b65_b.png)


上面比较简略的讲述了一下因子分析，下面再通过一个简单例子，来表述因子分析背后的思想。

假设我们有m=5个2维的样本点x(i)（两个特征），如下：
![](http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557474219.png)

那么按照因子分析的理解，样本点的生成过程如下：

1、 我们首先认为在1维空间（这里k=1），存在着按正态分布生成的m个点z(i)，如下

![](http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557499070.png)

均值为0，方差为1。

2、 然后使用某个![](http://images.cnblogs.com/cnblogs_com/jerrylead/201105/20110511155749989.png)将一维的z映射到2维，图形表示如下：

![](http://images.cnblogs.com/cnblogs_com/jerrylead/201105/20110511155750367.png)

3、 之后加上![](http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557505874.png)，即将所有点的横坐标移动u1，纵坐标移动u2，将直线移到一个位置，使得直线过点u，原始左边轴的原点现在为u（红色点）。

![](http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557566675.png)

然而，样本点不可能这么规则，在模型上会有一定偏差，因此我们需要将上步生成的点做一些扰动（误差），扰动![](http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557598820.png)。

4、 加入扰动后，我们得到黑色样本x(i)如下：

![](http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111558042959.png)

5、 其中由于z和![](http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111558052860.png)的均值都为0，因此u也是原始样本点（黑色点）的均值。

由以上的直观分析，我们知道了因子分析其实就是认为高维样本点实际上是由低维样本点经过高斯分布、线性变换、误差扰动生成的，因此高维数据可以使用低维来表示。

>在我们这篇博文中，我们只讲了降维方面，其实因子分析也是一个模型，但在这里不讲了，推荐一篇博文，有兴趣可以看下：[因子分析 ](http://blog.csdn.net/littleqqqqq/article/details/50899717)

## ICA（独立成分分析）

PCA是一种数据降维的方法，但是只对符合高斯分布的样本点比较有效，那么对于其他分布的样本，有没有主元分解的方法呢？来，下面我们来讲一个叫做独自成分分析的降维方式！

### 鸡尾酒宴会问题
* n个人，n个麦克风。从n个麦克风得到一组数据：![](https://danieljyc.github.io/img/1402665034566.png)。其中：i 表示采样的时间顺序，也就是说共得到了 m 组采样，每一组采样都是 n 维的。
* 我们的目标是单单从这 m 组采样数据中分辨出每个人说话的信号s。有 n 个信号源 ，![](https://danieljyc.github.io/img/1402665183196.png),s**相互独立。**
* A 是一个未知的混合矩阵（mixing matrix），用来组合叠加信号 s。

1. 我们可以得到：
![](https://danieljyc.github.io/img/1402665269996.png)
>其中， x 不是一个向量，是一个矩阵

2. 其中每个列向量
![](https://danieljyc.github.io/img/1402665319894.png)
![](https://danieljyc.github.io/img/1402665355695.png)

3. A 和 s 都是未知的，x 是已知的，我们要想办法根据 x 来推出 s。这个过程也称作为盲信号分离。
![](https://danieljyc.github.io/img/1402665589103.png)
![](https://danieljyc.github.io/img/1402665596364.png)


2. 最终得到：
![](https://danieljyc.github.io/img/1402665627938.png)

$s_{(i)}^{j}$：表示speaker j 在时刻i发出的信号。
对于此，我们需要知道两个量才能求出另外一个，下面我们进一步分析。

### ICA算法的前处理步骤
1. 中心化：也就是求 x 均值，然后让所有 x 减去均值，这一步与 PCA 一致。
2. 漂白：目的是为了让x相互独立。将 x 乘以一个矩阵变成 (其协方差矩阵是$I$)。
![](https://danieljyc.github.io/img/1402667667616.png)
其中，![](https://danieljyc.github.io/img/1402667709919.png)
其中使用特征值分解来得到 E（特征向量矩阵）和 D（特征值对角矩阵） ，计算公式为
![](https://danieljyc.github.io/img/1402667761686.png)

### ICA算法
1. 我们假定每$s_i$有概率密度$p_s$，那么给定时刻原信号的联合分布就是
![](https://danieljyc.github.io/img/1402666306377.png)
>注：每个人发出的声音信号s各自独立。

2. 然后，我们就可以求得p(x)

3. 现在，我们需要知道p(s)和w，才能求得p(x)。
首先，我们假设s 的累积分布函数符合 sigmoid 函数
![](https://danieljyc.github.io/img/1402666520656.png)

这就是 s 的密度函数。这里 s 是实数。

4. 	然后，我们就剩下W了。我们用最大似然估计的方法求解。
使用前面得到的 x 的概率密度函数，得
![](https://danieljyc.github.io/img/1402666734641.png)

![](https://danieljyc.github.io/img/1402666874964.png)
最终，我们求得：
![](https://danieljyc.github.io/img/1402666911972.png)
>其中α是梯度上升速率，人为指定。

5. 迭代求出 W 后，我们也可以还原出原始信号：

### IAC应用
如果把麦克风x换成采集脑电波的电极，信号源s就代表大脑独立进程：心跳、眨眼等。通过将信号x减去心跳、眨眼等无用信号，我们就可以得到大脑内部信号。

### 小结
* ICA 的盲信号分析领域的一个强有力方法，也是求非高斯分布数据隐含因子的方法。
* ICA和PCA对比：

>ICA: 从之前我们熟悉的样本-特征角度看，我们使用 ICA 的前提条件是，认为样本数据由独立非高斯分布的隐含因子产生，隐含因子个数等于特征数。更适合用来还原信号（因为信号比较有规律，经常不是高斯分布的）。
PCA : 认为特征是由 k 个正交的特征（也可看作是隐含因子）生成的。更适合用来降维（用那么多特征干嘛，k 个正交的即可）
有时候也需要组合两者一起使用。

最后：在这篇博文笔记中，我们学习了一些特征工程的方法，AOC和AUC，还有一些具体的降维技术，这些都是比较基础的，还有很多没有讲到的，现在先不讲了把，以后有机会再说（逃）

参考资料：


《统计学习方法》-李航<br>
《机器学习》-周志华<br>
《机器学习实战》-Peter Harrington<br>
斯坦福大学公开课-机器学习<br>
网上的各位大牛的博文<br>

  <!-- 多说评论框 start -->
  <div class="ds-thread" data-thread-key="20170211011" data-title="feature_engen" data-url=""></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"yzhhome"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共JS代码 end -->

